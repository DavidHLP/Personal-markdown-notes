# å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿç¯å¢ƒæ­å»ºæŒ‡å—

> ğŸš€ **ä»é›¶å¼€å§‹æ„å»ºç”Ÿäº§çº§å¤§æ•°æ®å¹³å°**
> æœ¬æŒ‡å—å°†å¸¦æ‚¨æ„å»ºåŒ…å« Hadoopã€HBaseã€Spark çš„å®Œæ•´é«˜å¯ç”¨å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿ

## ğŸ“‹ ç›®å½•

### ğŸš€ å¿«é€Ÿå¼€å§‹
- [æ¦‚è¿°](#æ¦‚è¿°)
  - [ç”Ÿæ€ç³»ç»Ÿæ¶æ„](#ç”Ÿæ€ç³»ç»Ÿæ¶æ„)
  - [æŠ€æœ¯æ ˆå¯¹æ¯”](#æŠ€æœ¯æ ˆå¯¹æ¯”)

### ğŸ“‹ ç¯å¢ƒå‡†å¤‡
- [å‰ç½®æ¡ä»¶](#å‰ç½®æ¡ä»¶)
  - [ç¡¬ä»¶è¦æ±‚](#ç¡¬ä»¶è¦æ±‚)
  - [è½¯ä»¶è¦æ±‚](#è½¯ä»¶è¦æ±‚)
  - [ç½‘ç»œæ¶æ„](#ç½‘ç»œæ¶æ„)
- [åŸºç¡€ç¯å¢ƒå‡†å¤‡](#åŸºç¡€ç¯å¢ƒå‡†å¤‡)
  - [Dockerç¯å¢ƒé…ç½®](#æ­¥éª¤-1docker-ç¯å¢ƒé…ç½®)
  - [è½¯ä»¶åŒ…ä¸‹è½½](#æ­¥éª¤-2ä¸‹è½½è½¯ä»¶åŒ…)
  - [ç½‘ç»œå’Œä¸»æœºé…ç½®](#æ­¥éª¤-3ç½‘ç»œå’Œä¸»æœºé…ç½®)

### ğŸ—ï¸ åˆ†é˜¶æ®µéƒ¨ç½²æŒ‡å—

#### é˜¶æ®µä¸€ï¼šHadoop åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
- [HDFSé«˜å¯ç”¨æ¶æ„](#hdfs-é«˜å¯ç”¨æ¶æ„)
- [æ­¥éª¤1ï¼šéƒ¨ç½²ZooKeeperé›†ç¾¤](#æ­¥éª¤-1éƒ¨ç½²-zookeeper-é›†ç¾¤)
- [æ­¥éª¤2ï¼šéƒ¨ç½²Hadoopé›†ç¾¤](#æ­¥éª¤-2éƒ¨ç½²-hadoop-é›†ç¾¤)
- [æ­¥éª¤3ï¼šHadoopé…ç½®æ–‡ä»¶](#æ­¥éª¤-3hadoop-é…ç½®æ–‡ä»¶)
  - [hadoop-env.shé…ç½®](#hadoop-envsh-é…ç½®)
  - [core-site.xmlé…ç½®](#core-sitexml-é…ç½®)
  - [hdfs-site.xml](#hdfs-sitexml)
  - [yarn-site.xml](#yarn-sitexml)
  - [mapred-site.xml](#mapred-sitexml)
  - [Workersé…ç½®](#workers-é…ç½®)
- [æ­¥éª¤4ï¼šå¯åŠ¨Hadoopé›†ç¾¤](#æ­¥éª¤-4å¯åŠ¨-hadoop-é›†ç¾¤)
- [æ­¥éª¤5ï¼šéªŒè¯Hadoopé›†ç¾¤](#æ­¥éª¤-5éªŒè¯-hadoop-é›†ç¾¤)

#### é˜¶æ®µäºŒï¼šHBase åˆ†å¸ƒå¼æ•°æ®åº“
- [HBaseæ¶æ„è®¾è®¡](#hbase-æ¶æ„è®¾è®¡)
- [æ­¥éª¤1ï¼šHBaseé…ç½®](#æ­¥éª¤-1hbase-é…ç½®)
  - [hbase-env.sh](#hbase-envsh)
  - [hbase-site.xml](#hbase-sitexml)
  - [RegionServerå’Œå¤‡ç”¨Masteré…ç½®](#regionserver-å’Œå¤‡ç”¨-master-é…ç½®)
- [æ­¥éª¤2ï¼šå¯åŠ¨HBase](#æ­¥éª¤-2å¯åŠ¨-hbase)

#### é˜¶æ®µä¸‰ï¼šSpark è®¡ç®—å¼•æ“
- [Sparké›†ç¾¤æ¶æ„](#spark-é›†ç¾¤æ¶æ„)
- [æ­¥éª¤1ï¼šSparké…ç½®](#æ­¥éª¤-2spark-é…ç½®)
  - [spark-defaults.conf](#spark-defaultsconf)
  - [spark-env.sh](#spark-envsh)
  - [Workersé…ç½®](#workers-é…ç½®-1)
- [æ­¥éª¤2ï¼šå¯åŠ¨Sparké›†ç¾¤](#æ­¥éª¤-3å¯åŠ¨-spark-é›†ç¾¤)
- [æ­¥éª¤3ï¼šSparkæ¨¡å¼é€‰æ‹©](#æ­¥éª¤-4spark-æ¨¡å¼é€‰æ‹©)
  - [Localæ¨¡å¼](#local-æ¨¡å¼å¼€å‘æµ‹è¯•)
  - [Standaloneæ¨¡å¼](#standalone-æ¨¡å¼é›†ç¾¤éƒ¨ç½²)
  - [PySparké›†ç¾¤æ¨¡å¼](#pyspark-é›†ç¾¤æ¨¡å¼)
- [æ­¥éª¤4ï¼šéªŒè¯Spark](#æ­¥éª¤-5éªŒè¯-spark)

### ğŸ”§ é›†ç¾¤éªŒè¯ä¸æµ‹è¯•
- [ç³»ç»Ÿé›†æˆæµ‹è¯•](#ç³»ç»Ÿé›†æˆæµ‹è¯•)
  - [å®Œæ•´é›†ç¾¤çŠ¶æ€æ£€æŸ¥](#å®Œæ•´é›†ç¾¤çŠ¶æ€æ£€æŸ¥)
  - [æ•°æ®æµæµ‹è¯•](#æ•°æ®æµæµ‹è¯•)
- [æ•…éšœè½¬ç§»æµ‹è¯•](#æ•…éšœè½¬ç§»æµ‹è¯•)
  - [NameNodeæ•…éšœè½¬ç§»](#namenode-æ•…éšœè½¬ç§»)

### ğŸ”§ ç›‘æ§ä¸è¿ç»´
- [ç›‘æ§æŒ‡æ ‡](#ç›‘æ§æŒ‡æ ‡)
- [æ—¥å¿—ç®¡ç†](#æ—¥å¿—ç®¡ç†)
  - [æ—¥å¿—æ”¶é›†è„šæœ¬](#æ—¥å¿—æ”¶é›†è„šæœ¬)
- [å¤‡ä»½ç­–ç•¥](#å¤‡ä»½ç­–ç•¥)
  - [HDFSæ•°æ®å¤‡ä»½](#hdfs-æ•°æ®å¤‡ä»½)

### ğŸ› ï¸ æ•…éšœæ’é™¤
- [å¸¸è§é—®é¢˜è§£å†³](#å¸¸è§é—®é¢˜è§£å†³)
  - [é›†ç¾¤å¯åŠ¨é—®é¢˜](#1-é›†ç¾¤å¯åŠ¨é—®é¢˜)
    - [ZooKeeperè¿æ¥å¤±è´¥](#zookeeperè¿æ¥å¤±è´¥)
    - [NameNodeæ— æ³•å¯åŠ¨](#namenodeæ— æ³•å¯åŠ¨)
  - [æ€§èƒ½é—®é¢˜](#2-æ€§èƒ½é—®é¢˜)
    - [å†…å­˜ä¸è¶³](#å†…å­˜ä¸è¶³)
    - [ç½‘ç»œå»¶è¿Ÿé«˜](#ç½‘ç»œå»¶è¿Ÿé«˜)
- [è¯Šæ–­å·¥å…·](#è¯Šæ–­å·¥å…·)
  - [é›†ç¾¤è¯Šæ–­è„šæœ¬](#é›†ç¾¤è¯Šæ–­è„šæœ¬)

### ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–
- [ç¡¬ä»¶ä¼˜åŒ–å»ºè®®](#ç¡¬ä»¶ä¼˜åŒ–å»ºè®®)
  - [å­˜å‚¨ä¼˜åŒ–](#å­˜å‚¨ä¼˜åŒ–)
- [è½¯ä»¶è°ƒä¼˜å‚æ•°](#è½¯ä»¶è°ƒä¼˜å‚æ•°)
  - [Hadoopè°ƒä¼˜](#hadoop-è°ƒä¼˜)
  - [HBaseè°ƒä¼˜](#hbase-è°ƒä¼˜)
  - [Sparkè°ƒä¼˜](#spark-è°ƒä¼˜)

### ğŸ¯ æ€»ç»“
- [æŠ€æœ¯æ¶æ„å›é¡¾](#æŠ€æœ¯æ¶æ„å›é¡¾)
- [å¹³å°èƒ½åŠ›çŸ©é˜µ](#å¹³å°èƒ½åŠ›çŸ©é˜µ)
- [åº”ç”¨åœºæ™¯å»ºè®®](#åº”ç”¨åœºæ™¯å»ºè®®)
- [è¿›é˜¶å­¦ä¹ è·¯å¾„](#è¿›é˜¶å­¦ä¹ è·¯å¾„)
- [ç”Ÿäº§å®è·µå»ºè®®](#ç”Ÿäº§å®è·µå»ºè®®)

## æ¦‚è¿°

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨æ„å»ºä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§å¤§æ•°æ®å¤„ç†å¹³å°ï¼ŒåŒ…å«å­˜å‚¨ã€è®¡ç®—å’Œæ•°æ®åº“ä¸‰å¤§æ ¸å¿ƒç»„ä»¶ã€‚

### ç”Ÿæ€ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    subgraph "æ•°æ®é‡‡é›†å±‚"
        A1[æ—¥å¿—æ–‡ä»¶]
        A2[å®æ—¶æ•°æ®æµ]
        A3[å…³ç³»æ•°æ®åº“]
        A4[APIæ¥å£]
    end

    subgraph "æ•°æ®å­˜å‚¨å±‚"
        B1[HDFSåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ]
        B2[HBaseåˆ†å¸ƒå¼æ•°æ®åº“]
    end

    subgraph "æ•°æ®è®¡ç®—å±‚"
        C1[Sparkæ‰¹å¤„ç†]
        C2[Sparkæµå¤„ç†]
        C3[Spark SQL]
        C4[Spark MLlib]
    end

    subgraph "æ•°æ®åº”ç”¨å±‚"
        D1[æ•°æ®åˆ†æ]
        D2[æœºå™¨å­¦ä¹ ]
        D3[å®æ—¶ç›‘æ§]
        D4[æ•°æ®å¯è§†åŒ–]
    end

    subgraph "åè°ƒæœåŠ¡"
        E1[ZooKeeperé›†ç¾¤]
    end

    A1 --> B1
    A2 --> B2
    A3 --> B1
    A4 --> B2

    B1 --> C1
    B1 --> C2
    B2 --> C3
    B2 --> C4

    C1 --> D1
    C2 --> D3
    C3 --> D2
    C4 --> D4

    E1 -.-> B1
    E1 -.-> B2
    E1 -.-> C1

    style A1 fill:#e1f5fe
    style B1 fill:#fff3e0
    style C1 fill:#e8f5e8
    style D1 fill:#f3e5f5
    style E1 fill:#ffcdd2
```

### æŠ€æœ¯æ ˆå¯¹æ¯”

| ç»„ä»¶                  | ä½œç”¨         | é«˜å¯ç”¨ | æ‰©å±•æ€§     | å¤æ‚åº¦   |
| --------------------- | ------------ | ------ | ---------- | -------- |
| **Hadoop HDFS** | åˆ†å¸ƒå¼å­˜å‚¨   | âœ…     | â­â­â­â­â­ | â­â­â­   |
| **HBase**       | NoSQL æ•°æ®åº“ | âœ…     | â­â­â­â­   | â­â­â­â­ |
| **Spark**       | è®¡ç®—å¼•æ“     | âœ…     | â­â­â­â­â­ | â­â­â­   |
| **ZooKeeper**   | åè°ƒæœåŠ¡     | âœ…     | â­â­â­     | â­â­     |

## å‰ç½®æ¡ä»¶

### ç¡¬ä»¶è¦æ±‚

#### é›†ç¾¤èµ„æºè§„åˆ’

| èŠ‚ç‚¹ç±»å‹       | æ•°é‡ | CPU   | å†…å­˜  | ç£ç›˜   | ç½‘ç»œ  |
| -------------- | ---- | ----- | ----- | ------ | ----- |
| Master èŠ‚ç‚¹    | 3    | 4 æ ¸+ | 8GB+  | 100GB+ | åƒå…†+ |
| Worker èŠ‚ç‚¹    | 3+   | 8 æ ¸+ | 16GB+ | 500GB+ | åƒå…†+ |
| ZooKeeper èŠ‚ç‚¹ | 3    | 2 æ ¸+ | 4GB+  | 50GB+  | åƒå…†+ |

### è½¯ä»¶è¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Linux (Ubuntu 20.04+ / CentOS 8+ / RHEL 8+)
- **å®¹å™¨åŒ–**: Docker 20.10+ / Docker Compose 2.0+
- **Java**: OpenJDK 8 æˆ– 11
- **Python**: Python 3.8+ (ç”¨äº Spark)

### ç½‘ç»œæ¶æ„

```mermaid
graph LR
    subgraph "ZooKeeperé›†ç¾¤"
        Z1[zoo1<br/>10.10.1.10:2181]
        Z2[zoo2<br/>10.10.1.11:2182]
        Z3[zoo3<br/>10.10.1.12:2183]
    end

    subgraph "Hadoop MasterèŠ‚ç‚¹"
        M1[hadoop-master1<br/>10.10.1.20]
        M2[hadoop-master2<br/>10.10.1.21]
        M3[hadoop-master3<br/>10.10.1.22]
    end

    subgraph "Hadoop WorkerèŠ‚ç‚¹"
        W1[hadoop-worker1<br/>10.10.1.23]
        W2[hadoop-worker2<br/>10.10.1.24]
        W3[hadoop-worker3<br/>10.10.1.25]
    end

    Z1 -.-> M1
    Z2 -.-> M2
    Z3 -.-> M3

    M1 --> W1
    M1 --> W2
    M1 --> W3

    style Z1 fill:#e8f5e8
    style Z2 fill:#e8f5e8
    style Z3 fill:#e8f5e8
    style M1 fill:#ffcdd2
    style M2 fill:#e0e0e0
    style M3 fill:#e0e0e0
    style W1 fill:#e1f5fe
    style W2 fill:#e1f5fe
    style W3 fill:#e1f5fe
```

## åŸºç¡€ç¯å¢ƒå‡†å¤‡

### æ­¥éª¤ 1ï¼šDocker ç¯å¢ƒé…ç½®

#### åˆ›å»ºåŸºç¡€é•œåƒ

æ ¹æ®æ‚¨çš„æ¶æ„é€‰æ‹©ç›¸åº”çš„ Dockerfileï¼š

#### AMD64æ¶æ„ Dockerfile

```dockerfile
FROM ubuntu:22.04

# ç¯å¢ƒå˜é‡è®¾ç½®
ENV HADOOP_HOME=/opt/hadoop
ENV HBASE_HOME=/opt/hbase
ENV SPARK_HOME=/opt/spark
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

# ä»¥ root ç”¨æˆ·æ‰§è¡Œ
USER root

# æ›´æ–°å¹¶å®‰è£…ä¾èµ–åŒ…
RUN apt-get update && \
    apt-get install -y sudo openjdk-8-jdk openssh-server openssh-client \
                       wget curl vim net-tools telnet && \
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    mkdir -p /data/hdfs && \
    mkdir -p /data/hdfs/journal/node/local/data

# ä¸‹è½½å¹¶å®‰è£…Miniconda
RUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh && \
    chmod +x /tmp/miniconda.sh && \
    /tmp/miniconda.sh -b -p /opt/miniconda3 && \
    rm /tmp/miniconda.sh

# è®¾ç½®Minicondaç¯å¢ƒå˜é‡
ENV PATH="/opt/miniconda3/bin:$PATH"

# å¯åŠ¨ SSH æœåŠ¡
RUN service ssh start

# æš´éœ²ç«¯å£
EXPOSE 9870 9868 9864 9866 8088 8020 16000 16010 16020 7077 8080 8081 22

# å®¹å™¨å¯åŠ¨æ—¶å¯åŠ¨ SSH
CMD ["/usr/sbin/sshd", "-D"]
```

#### ARM64æ¶æ„ Dockerfile

```dockerfile
FROM ubuntu:22.04

# ç¯å¢ƒå˜é‡è®¾ç½®
ENV HADOOP_HOME=/opt/hadoop
ENV HBASE_HOME=/opt/hbase
ENV SPARK_HOME=/opt/spark
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-arm64

# ä»¥ root ç”¨æˆ·æ‰§è¡Œ
USER root

# æ›´æ–°å¹¶å®‰è£…ä¾èµ–åŒ…
RUN apt-get update && \
    apt-get install -y sudo openjdk-8-jdk openssh-server openssh-client \
                       wget curl vim net-tools telnet && \
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    mkdir -p /data/hdfs && \
    mkdir -p /data/hdfs/journal/node/local/data

# ä¸‹è½½å¹¶å®‰è£…Miniconda
RUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh && \
    chmod +x /tmp/miniconda.sh && \
    /tmp/miniconda.sh -b -p /opt/miniconda3 && \
    rm /tmp/miniconda.sh

# è®¾ç½®Minicondaç¯å¢ƒå˜é‡
ENV PATH="/opt/miniconda3/bin:$PATH"

# å¯åŠ¨ SSH æœåŠ¡
RUN service ssh start

# æš´éœ²ç«¯å£
EXPOSE 9870 9868 9864 9866 8088 8020 16000 16010 16020 7077 8080 8081 22

# å®¹å™¨å¯åŠ¨æ—¶å¯åŠ¨ SSH
CMD ["/usr/sbin/sshd", "-D"]
```

#### æ„å»ºé•œåƒ

```bash
# æ„å»ºåŸºç¡€é•œåƒ
docker build -t bigdata-platform:1.0 .
```

### æ­¥éª¤ 2ï¼šä¸‹è½½è½¯ä»¶åŒ…

åˆ›å»ºè½¯ä»¶ä¸‹è½½è„šæœ¬ï¼š

> [!NOTE]
> æ–‡ä»¶å: download-packages.sh

```bash
#!/bin/bash

# åˆ›å»ºä¸‹è½½ç›®å½•
mkdir -p ~/opt/docker-data/hadoop-hbase-spark
cd ~/opt/docker-data/hadoop-hbase-spark

# ä¸‹è½½Hadoop
echo "ä¸‹è½½Hadoop..."
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz
tar -xzf hadoop-3.4.0.tar.gz

# ä¸‹è½½HBase
echo "ä¸‹è½½HBase..."
wget https://archive.apache.org/dist/hbase/2.5.10/hbase-2.5.10-hadoop3-bin.tar.gz
tar -xzf hbase-2.5.10-hadoop3-bin.tar.gz

# ä¸‹è½½Spark
echo "ä¸‹è½½Spark..."
wget https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
tar -xzf spark-3.4.1-bin-hadoop3.tgz

# ä¸‹è½½Miniconda
echo "ä¸‹è½½Miniconda..."
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
# è®¾ç½®æƒé™
chmod -R 755 ~/opt/docker-data/hadoop-hbase-spark
echo "è½¯ä»¶åŒ…ä¸‹è½½å®Œæˆ!"

#è§£å‹æ–‡ä»¶
tar -xzf hadoop-3.4.0.tar.gz && mv hadoop-3.4.0 hadoop
tar -xzf hbase-2.5.10-hadoop3-bin.tar.gz && mv hbase-2.5.10-hadoop3 hbase
tar -xzf spark-3.4.1-bin-hadoop3.tgz && mv spark-3.4.1-bin-hadoop3 spark
echo "è§£å‹æ–‡ä»¶å®Œæˆ!"
```

### æ­¥éª¤ 3ï¼šç½‘ç»œå’Œä¸»æœºé…ç½®

#### é…ç½® hosts æ–‡ä»¶

```bash
# åœ¨æ‰€æœ‰èŠ‚ç‚¹çš„/etc/hostsæ–‡ä»¶ä¸­æ·»åŠ 
10.10.1.10 zoo1
10.10.1.11 zoo2
10.10.1.12 zoo3
10.10.1.20 hadoop-master1
10.10.1.21 hadoop-master2
10.10.1.22 hadoop-master3
10.10.1.23 hadoop-worker1
10.10.1.24 hadoop-worker2
10.10.1.25 hadoop-worker3
```

## é˜¶æ®µä¸€ï¼šHadoop åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ

Hadoop HDFS æ˜¯æ•´ä¸ªå¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿçš„å­˜å‚¨åŸºç¡€ï¼Œæä¾›é«˜å¯ç”¨ã€é«˜å¯é çš„åˆ†å¸ƒå¼å­˜å‚¨èƒ½åŠ›ã€‚

### HDFS é«˜å¯ç”¨æ¶æ„

```mermaid
graph TB
    subgraph "ZooKeeperåè°ƒå±‚"
        ZK1[ZooKeeper-1]
        ZK2[ZooKeeper-2]
        ZK3[ZooKeeper-3]
    end

    subgraph "HDFSç®¡ç†å±‚"
        NN1[NameNode-1<br/>Active]
        NN2[NameNode-2<br/>Standby]
        NN3[NameNode-3<br/>Standby]

        ZKFC1[ZKFC-1]
        ZKFC2[ZKFC-2]
        ZKFC3[ZKFC-3]
    end

    subgraph "å…±äº«å­˜å‚¨å±‚"
        JN1[JournalNode-1]
        JN2[JournalNode-2]
        JN3[JournalNode-3]
    end

    subgraph "æ•°æ®å­˜å‚¨å±‚"
        DN1[DataNode-1]
        DN2[DataNode-2]
        DN3[DataNode-3]
    end

    ZK1 -.-> ZKFC1
    ZK2 -.-> ZKFC2
    ZK3 -.-> ZKFC3

    ZKFC1 --> NN1
    ZKFC2 --> NN2
    ZKFC3 --> NN3

    NN1 --> JN1
    NN1 --> JN2
    NN1 --> JN3

    NN2 -.-> JN1
    NN2 -.-> JN2
    NN2 -.-> JN3

    NN1 --> DN1
    NN1 --> DN2
    NN1 --> DN3

    style NN1 fill:#ffcdd2
    style NN2 fill:#e0e0e0
    style NN3 fill:#e0e0e0
    style ZK1 fill:#e8f5e8
    style ZK2 fill:#e8f5e8
    style ZK3 fill:#e8f5e8
```

### æ­¥éª¤ 1ï¼šéƒ¨ç½² ZooKeeper é›†ç¾¤

#### ZooKeeper Docker Compose é…ç½®

> [!NOTE]
> æ–‡ä»¶å: zookeeper-compose.yml

```yaml
services:
  zoo1:
    image: zookeeper:3.7.1-temurin
    container_name: zoo1
    restart: always
    hostname: zoo1
    ports:
      - 2181:2181
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
    volumes:
      - ~/opt/docker-data/hadoop-hbase-spark/zoo1/data:/data
      - ~/opt/docker-data/hadoop-hbase-spark/zoo1/logs:/datalog
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.10

  zoo2:
    image: zookeeper:3.7.1-temurin
    container_name: zoo2
    restart: always
    hostname: zoo2
    ports:
      - 2182:2181
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
    volumes:
      - ~/opt/docker-data/hadoop-hbase-spark/zoo2/data:/data
      - ~/opt/docker-data/hadoop-hbase-spark/zoo2/logs:/datalog
    networks:
       zookeeper-cluster:
        ipv4_address: 10.10.1.11

  zoo3:
    image: zookeeper:3.7.1-temurin
    container_name: zoo3
    restart: always
    hostname: zoo3
    ports:
      - 2183:2181
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
    volumes:
      - ~/opt/docker-data/hadoop-hbase-spark/zoo3/data:/data
      - ~/opt/docker-data/hadoop-hbase-spark/zoo3/logs:/datalog
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.12

networks:
  zookeeper-cluster:
    name: zookeeper-cluster
    external: true
    ipam:
      config:
        - subnet: "10.10.1.0/24"
```

#### å¯åŠ¨ ZooKeeper é›†ç¾¤

```bash
# å¯åŠ¨ZooKeeperé›†ç¾¤
docker-compose -f zookeeper-compose.yml up -d

# éªŒè¯ZooKeeperé›†ç¾¤çŠ¶æ€
docker exec zoo1 /apache-zookeeper-3.7.1-bin/bin/zkServer.sh status
docker exec zoo2 /apache-zookeeper-3.7.1-bin/bin/zkServer.sh status
docker exec zoo3 /apache-zookeeper-3.7.1-bin/bin/zkServer.sh status
```

### æ­¥éª¤ 2ï¼šéƒ¨ç½² Hadoop é›†ç¾¤

#### Hadoop é›†ç¾¤ Docker Compose é…ç½®

> [!NOTE]
> æ–‡ä»¶å: hadoop-compose.yml

```yaml
# å®šä¹‰å…¬å…±é…ç½®é”šç‚¹
x-hadoop: &hadoop-common
  image: big-data-components:1.0
  stdin_open: true
  tty: true
  command: sh -c "/usr/sbin/sshd -D"
  networks:
    zookeeper-cluster:
  volumes: &hadoop-volumes
    - type: bind
      source: ~/opt/docker-data/hadoop-hbase-spark/hadoop
      target: /opt/hadoop
    - type: bind
      source: ~/opt/docker-data/hadoop-hbase-spark/hbase
      target: /opt/hbase
    - type: bind
      source: ~/opt/docker-data/hadoop-hbase-spark/spark
      target: /opt/spark
    - /etc/localtime:/etc/localtime:ro
    - /etc/timezone:/etc/timezone:ro
  environment: &hadoop-env
    JAVA_HOME: "/usr/lib/jvm/java-8-openjdk-amd64"
    HADOOP_HOME: "/opt/hadoop"
    HADOOP_COMMON_HOME: "/opt/hadoop"
    HADOOP_HDFS_HOME: "/opt/hadoop"
    HADOOP_MAPRED_HOME: "/opt/hadoop"
    YARN_HOME: "/opt/hadoop"
    HBASE_HOME: "/opt/hbase"
    SPARK_HOME: "/opt/spark"
    PYTHON_HOME: "/opt/miniconda3"
    HADOOP_CONF_DIR: "/opt/hadoop/etc/hadoop"
    HBASE_CONF_DIR: "/opt/hbase/conf"
    SPARK_CONF_DIR: "/opt/spark/conf"
    PYSPARK_PYTHON: "/opt/miniconda3/bin/python"
    PYSPARK_DRIVER_PYTHON: "/opt/miniconda3/bin/python"
    PYTHONPATH: "/opt/miniconda3/lib/python3.8/site-packages"
    PYTHONIOENCODING: "utf-8"
    CLASSPATH: ".:/usr/lib/jvm/java-8-openjdk-amd64/lib/dt.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar"
    PATH: "/opt/miniconda3/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin:/opt/hadoop/bin:/opt/hadoop/sbin:/opt/hbase/bin:/opt/spark/bin:/opt/spark/sbin:${PATH}"
    # Hadoopç”¨æˆ·æƒé™é…ç½® - è§£å†³rootç”¨æˆ·è¿è¡Œé”™è¯¯
    HDFS_NAMENODE_USER: "root"
    HDFS_DATANODE_USER: "root"
    HDFS_JOURNALNODE_USER: "root"
    HDFS_ZKFC_USER: "root"
    YARN_RESOURCEMANAGER_USER: "root"
    YARN_NODEMANAGER_USER: "root"
    YARN_PROXYSERVER_USER: "root"
    MAPRED_HISTORYSERVER_USER: "root"

services:
  # Hadoop ä¸»èŠ‚ç‚¹é…ç½®
  hadoop-master1: &hadoop-master
    <<: *hadoop-common
    container_name: hadoop-master1
    hostname: hadoop-master1
    volumes: *hadoop-volumes
    environment: *hadoop-env
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.20
    restart: no
    
  hadoop-master2:
    <<: *hadoop-master
    container_name: hadoop-master2
    hostname: hadoop-master2
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.21
    restart: no
    
  hadoop-master3:
    <<: *hadoop-master
    container_name: hadoop-master3
    hostname: hadoop-master3
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.22
    restart: no
    
  hadoop-worker1:
    <<: *hadoop-common
    container_name: hadoop-worker1
    hostname: hadoop-worker1
    environment: *hadoop-env
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.23
    restart: no
    
  hadoop-worker2:
    <<: *hadoop-common
    container_name: hadoop-worker2
    hostname: hadoop-worker2
    environment: *hadoop-env
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.24
    restart: no
    
  hadoop-worker3:
    <<: *hadoop-common
    container_name: hadoop-worker3
    hostname: hadoop-worker3
    environment: *hadoop-env
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.25
    restart: no

networks:
  zookeeper-cluster:
    name: zookeeper-cluster
    external: true
    ipam:
      config:
        - subnet: "10.10.1.0/24"
```

### æ­¥éª¤ 3ï¼šHadoop é…ç½®æ–‡ä»¶

#### æ ¸å¿ƒé…ç½®æ–‡ä»¶è¯¦è§£

#### hadoop-env.sh é…ç½®

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/hadoop/etc/hadoop/hadoop-env.sh

```bash
# Javaè·¯å¾„é…ç½®
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

# Hadoopç”¨æˆ·é…ç½®
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root

# å†…å­˜é…ç½®
export HADOOP_HEAPSIZE=2048m
export HADOOP_NAMENODE_INIT_HEAPSIZE=2048m
```

#### core-site.xml é…ç½®

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/hadoop/etc/hadoop/core-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <!-- è®¾ç½®é»˜è®¤æ–‡ä»¶ç³»ç»Ÿä¸ºé«˜å¯ç”¨çš„HDFS -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://mycluster</value>
    </property>

    <!-- Hadoopä¸´æ—¶ç›®å½• -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/tmp/hadoop-${user.name}</value>
    </property>

    <!-- ZooKeeperé…ç½® -->
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>zoo1:2181,zoo2:2181,zoo3:2181</value>
    </property>

    <!-- ä»£ç†ç”¨æˆ·é…ç½® -->
    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
    </property>
</configuration>
```

#### hdfs-site.xml

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/hadoop/etc/hadoop/hdfs-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <!-- å‘½åæœåŠ¡é…ç½® -->
    <property>
        <name>dfs.nameservices</name>
        <value>mycluster</value>
    </property>

    <!-- NameNodeé«˜å¯ç”¨é…ç½® -->
    <property>
        <name>dfs.ha.namenodes.mycluster</name>
        <value>nn1,nn2,nn3</value>
    </property>

    <!-- NameNode RPCåœ°å€ -->
    <property>
        <name>dfs.namenode.rpc-address.mycluster.nn1</name>
        <value>hadoop-master1:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.mycluster.nn2</name>
        <value>hadoop-master2:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.mycluster.nn3</name>
        <value>hadoop-master3:8020</value>
    </property>

    <!-- NameNode HTTPåœ°å€ -->
    <property>
        <name>dfs.namenode.http-address.mycluster.nn1</name>
        <value>hadoop-master1:9870</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.mycluster.nn2</name>
        <value>hadoop-master2:9870</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.mycluster.nn3</name>
        <value>hadoop-master3:9870</value>
    </property>

    <!-- å…±äº«ç¼–è¾‘æ—¥å¿—é…ç½® -->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://hadoop-master1:8485;hadoop-master2:8485;hadoop-master3:8485/mycluster</value>
    </property>

    <!-- JournalNodeé…ç½® -->
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/data/hdfs/journal/node/local/data</value>
    </property>

    <!-- å®¢æˆ·ç«¯æ•…éšœè½¬ç§»é…ç½® -->
    <property>
        <name>dfs.client.failover.proxy.provider.mycluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>

    <!-- è‡ªåŠ¨æ•…éšœè½¬ç§» -->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>

    <!-- éš”ç¦»æœºåˆ¶ -->
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_rsa</value>
    </property>

    <!-- æ•°æ®å­˜å‚¨é…ç½® -->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/data/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/data/hdfs/datanode</value>
    </property>

    <!-- å¤åˆ¶å› å­ -->
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>

    <!-- æƒé™æ£€æŸ¥ -->
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
</configuration>
```

#### yarn-site.xml

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/hadoop/etc/hadoop/yarn-site.xml

```xml
<?xml version="1.0"?>
<configuration>
    <!-- ResourceManageré«˜å¯ç”¨é…ç½® -->
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>cluster1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2,rm3</value>
    </property>

    <!-- ResourceManageråœ°å€é…ç½® -->
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>hadoop-master1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>hadoop-master2</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm3</name>
        <value>hadoop-master3</value>
    </property>

    <!-- ZooKeeperé…ç½® -->
    <property>
        <name>hadoop.zk.address</name>
        <value>zoo1:2181,zoo2:2181,zoo3:2181</value>
    </property>

    <!-- NodeManageré…ç½® -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>8192</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>4</value>
    </property>

    <!-- è°ƒåº¦å™¨é…ç½® -->
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    </property>
</configuration>
```

#### mapred-site.xml

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/hadoop/etc/hadoop/mapred-site.xml

```xml
<?xml version="1.0"?>
<configuration>
    <!-- MapReduceæ¡†æ¶é…ç½® -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

    <!-- JobHistory Serveré«˜å¯ç”¨é…ç½® -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>0.0.0.0:10020</value>
        <description>JobHistory Server RPCåœ°å€ï¼Œä½¿ç”¨0.0.0.0ç›‘å¬æ‰€æœ‰æ¥å£</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>0.0.0.0:19888</value>
        <description>JobHistory Server Web UIåœ°å€</description>
    </property>

    <!-- JobHistoryå­˜å‚¨ç›®å½•é…ç½® - ä½¿ç”¨HDFSé«˜å¯ç”¨ -->
    <property>
        <name>mapreduce.jobhistory.done-dir</name>
        <value>hdfs://mycluster/mr-history/done</value>
        <description>å·²å®Œæˆä½œä¸šçš„å†å²æ–‡ä»¶å­˜å‚¨ç›®å½•</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.intermediate-done-dir</name>
        <value>hdfs://mycluster/mr-history/tmp</value>
        <description>ä¸­é—´å†å²æ–‡ä»¶å­˜å‚¨ç›®å½•</description>
    </property>

    <!-- è‡ªåŠ¨åˆ›å»ºç›®å½•é…ç½® -->
    <property>
        <name>mapreduce.jobhistory.move.interval-ms</name>
        <value>180000</value>
        <description>JobHistoryç§»åŠ¨æ–‡ä»¶çš„é—´éš”æ—¶é—´ï¼Œå¯ç”¨åä¼šè‡ªåŠ¨åˆ›å»ºç›®å½•</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.move.thread-count</name>
        <value>3</value>
        <description>JobHistoryç§»åŠ¨æ–‡ä»¶çš„çº¿ç¨‹æ•°</description>
    </property>
    
    <!-- å¯åŠ¨æ—¶æ£€æŸ¥å’Œåˆ›å»ºç›®å½• -->
    <property>
        <name>mapreduce.jobhistory.recovery.enable</name>
        <value>true</value>
        <description>å¯ç”¨JobHistoryæ¢å¤åŠŸèƒ½ï¼Œè‡ªåŠ¨å¤„ç†ç›®å½•é—®é¢˜</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.recovery.store.class</name>
        <value>org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService</value>
        <description>ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨çŠ¶æ€ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºå¿…è¦ç›®å½•</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.recovery.store.fs.uri</name>
        <value>hdfs://mycluster/mr-history/recovery</value>
        <description>æ¢å¤çŠ¶æ€å­˜å‚¨ç›®å½•</description>
    </property>

    <!-- åº”ç”¨ç¯å¢ƒé…ç½® -->
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
    </property>

    <!-- åˆ†å¸ƒå¼é›†ç¾¤æ€§èƒ½ä¼˜åŒ–é…ç½® -->
    <property>
        <name>mapreduce.job.ubertask.enable</name>
        <value>true</value>
        <description>å¯ç”¨uberä»»åŠ¡ä¼˜åŒ–å°ä½œä¸šæ€§èƒ½</description>
    </property>
    <property>
        <name>mapreduce.job.ubertask.maxmaps</name>
        <value>9</value>
        <description>uberä»»åŠ¡çš„æœ€å¤§mapæ•°é‡</description>
    </property>
    <property>
        <name>mapreduce.job.ubertask.maxreduces</name>
        <value>1</value>
        <description>uberä»»åŠ¡çš„æœ€å¤§reduceæ•°é‡</description>
    </property>

    <!-- Mapä»»åŠ¡é…ç½® -->
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>2048</value>
        <description>Mapä»»åŠ¡å†…å­˜é…ç½®</description>
    </property>
    <property>
        <name>mapreduce.map.java.opts</name>
        <value>-Xmx1638m</value>
        <description>Mapä»»åŠ¡JVMå‚æ•°</description>
    </property>

    <!-- Reduceä»»åŠ¡é…ç½® -->
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>2048</value>
        <description>Reduceä»»åŠ¡å†…å­˜é…ç½®</description>
    </property>
    <property>
        <name>mapreduce.reduce.java.opts</name>
        <value>-Xmx1638m</value>
        <description>Reduceä»»åŠ¡JVMå‚æ•°</description>
    </property>

    <!-- ApplicationMasteré…ç½® -->
    <property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <value>1024</value>
        <description>ApplicationMasterå†…å­˜é…ç½®</description>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.command-opts</name>
        <value>-Xmx819m</value>
        <description>ApplicationMaster JVMå‚æ•°</description>
    </property>

    <!-- ä½œä¸šå†å²è®°å½•ä¿ç•™é…ç½® -->
    <property>
        <name>mapreduce.jobhistory.max-age-ms</name>
        <value>604800000</value>
        <description>ä½œä¸šå†å²ä¿ç•™æ—¶é—´ï¼ˆ7å¤©ï¼‰</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.cleaner.enable</name>
        <value>true</value>
        <description>å¯ç”¨å†å²æ–‡ä»¶æ¸…ç†</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.cleaner.interval-ms</name>
        <value>86400000</value>
        <description>å†å²æ–‡ä»¶æ¸…ç†é—´éš”ï¼ˆ1å¤©ï¼‰</description>
    </property>

    <!-- å‹ç¼©é…ç½® -->
    <property>
        <name>mapreduce.map.output.compress</name>
        <value>true</value>
        <description>å¯ç”¨Mapè¾“å‡ºå‹ç¼©</description>
    </property>
    <property>
        <name>mapreduce.map.output.compress.codec</name>
        <value>org.apache.hadoop.io.compress.SnappyCodec</value>
        <description>Mapè¾“å‡ºå‹ç¼©ç¼–è§£ç å™¨</description>
    </property>
    <property>
        <name>mapreduce.output.fileoutputformat.compress</name>
        <value>true</value>
        <description>å¯ç”¨æœ€ç»ˆè¾“å‡ºå‹ç¼©</description>
    </property>
    <property>
        <name>mapreduce.output.fileoutputformat.compress.codec</name>
        <value>org.apache.hadoop.io.compress.SnappyCodec</value>
        <description>æœ€ç»ˆè¾“å‡ºå‹ç¼©ç¼–è§£ç å™¨</description>
    </property>
</configuration>
```

#### Workers é…ç½®

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/hadoop/etc/hadoop/workers

```bash
hadoop-worker1
hadoop-worker2
hadoop-worker3
```

### æ­¥éª¤ 4ï¼šå¯åŠ¨ Hadoop é›†ç¾¤

> [!NOTE]
> æ–‡ä»¶å: hadoop-init.sh

#### é›†ç¾¤åˆå§‹åŒ–è„šæœ¬

```bash
#!/bin/bash

#######################################################################
# Hadoop é›†ç¾¤é«˜å¯ç”¨(HA)åˆå§‹åŒ–è„šæœ¬
# 
# åŠŸèƒ½è¯´æ˜ï¼š
# 1. é…ç½®SSHå…å¯†ç™»å½•
# 2. å¯åŠ¨JournalNodeæœåŠ¡ 
# 3. æ ¼å¼åŒ–NameNodeå¹¶é…ç½®Standby
# 4. åˆå§‹åŒ–ZooKeeperæ•…éšœåˆ‡æ¢æ§åˆ¶å™¨
# 5. å¯åŠ¨Hadoopåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿå’ŒYARN
#
# ä½¿ç”¨æ–¹æ³•ï¼šbash hadoop-init.sh
# ä½œè€…ï¼šDavidHLP
# ç‰ˆæœ¬ï¼š1.0
#######################################################################

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# æ—¥å¿—æ–‡ä»¶é…ç½®
LOG_DIR="$(pwd)"
LOG_FILE="${LOG_DIR}/hadoop-init-$(date '+%Y%m%d_%H%M%S').log"

# åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
init_log() {
    echo "=====================================" > "$LOG_FILE"
    echo "Hadoop é›†ç¾¤åˆå§‹åŒ–æ—¥å¿—" >> "$LOG_FILE"
    echo "å¼€å§‹æ—¶é—´: $(date '+%Y-%m-%d %H:%M:%S')" >> "$LOG_FILE"
    echo "æ—¥å¿—æ–‡ä»¶: $LOG_FILE" >> "$LOG_FILE"
    echo "=====================================" >> "$LOG_FILE"
    echo "" >> "$LOG_FILE"
}

# æ—¥å¿—å‡½æ•° - åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶
log_info() {
    local msg="[INFO] $(date '+%Y-%m-%d %H:%M:%S') - $1"
    echo -e "${BLUE}${msg}${NC}"
    echo "$msg" >> "$LOG_FILE"
}

log_success() {
    local msg="[SUCCESS] $(date '+%Y-%m-%d %H:%M:%S') - $1"
    echo -e "${GREEN}${msg}${NC}"
    echo "$msg" >> "$LOG_FILE"
}

log_warning() {
    local msg="[WARNING] $(date '+%Y-%m-%d %H:%M:%S') - $1"
    echo -e "${YELLOW}${msg}${NC}"
    echo "$msg" >> "$LOG_FILE"
}

log_error() {
    local msg="[ERROR] $(date '+%Y-%m-%d %H:%M:%S') - $1"
    echo -e "${RED}${msg}${NC}"
    echo "$msg" >> "$LOG_FILE"
}

log_step() {
    local step_msg="æ­¥éª¤ $1: $2"
    echo -e "\n${PURPLE}========================================${NC}"
    echo -e "${PURPLE}${step_msg}${NC}"
    echo -e "${PURPLE}========================================${NC}"
    
    echo "" >> "$LOG_FILE"
    echo "========================================" >> "$LOG_FILE"
    echo "$step_msg" >> "$LOG_FILE"
    echo "========================================" >> "$LOG_FILE"
}

# æ£€æŸ¥å®¹å™¨çŠ¶æ€
check_container() {
    if docker ps --format "table {{.Names}}" | grep -q "$1"; then
        return 0
    else
        return 1
    fi
}

# æ‰§è¡Œå‘½ä»¤å¹¶è®°å½•è¾“å‡ºåˆ°æ—¥å¿—
exec_and_log() {
    local cmd="$1"
    local description="$2"
    
    if [ -n "$description" ]; then
        log_info "æ‰§è¡Œ: $description"
        echo "[COMMAND] $description" >> "$LOG_FILE"
    fi
    
    echo "[CMD] $cmd" >> "$LOG_FILE"
    
    # æ‰§è¡Œå‘½ä»¤å¹¶æ•è·è¾“å‡º
    local output
    output=$(eval "$cmd" 2>&1)
    local exit_code=$?
    
    # è®°å½•è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
    if [ -n "$output" ]; then
        echo "[OUTPUT] $output" >> "$LOG_FILE"
    fi
    
    echo "[EXIT_CODE] $exit_code" >> "$LOG_FILE"
    echo "" >> "$LOG_FILE"
    
    return $exit_code
}

echo -e "${CYAN}"
echo "  _   _           _                   _____       _ _   "
echo " | | | |         | |                 |_   _|     (_) |  "
echo " | |_| | __ _  __| | ___   ___  _ __   | |  _ __  _| |_ "
echo " |  _  |/ _\` |/ _\` |/ _ \ / _ \| '_ \  | | | '_ \| | __|"
echo " | | | | (_| | (_| | (_) | (_) | |_) |_| |_| | | | | |_ "
echo " |_| |_|\__,_|\__,_|\___/ \___/| .__/|_____|_| |_|_|\__|"
echo "                              | |                      "
echo "                              |_|                      "
echo -e "${NC}"
echo -e "${CYAN}Hadoop é«˜å¯ç”¨é›†ç¾¤åˆå§‹åŒ–è„šæœ¬å¯åŠ¨...${NC}\n"

# åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
init_log
log_info "Hadoop é«˜å¯ç”¨é›†ç¾¤åˆå§‹åŒ–è„šæœ¬å¯åŠ¨..."
log_info "æ—¥å¿—æ–‡ä»¶ä½ç½®: $LOG_FILE"

# å¯åŠ¨Dockerå®¹å™¨
log_step "0" "å¯åŠ¨Hadoopé›†ç¾¤å®¹å™¨"
log_info "ä½¿ç”¨docker-composeå¯åŠ¨æ‰€æœ‰å®¹å™¨..."
exec_and_log "docker-compose -f hadoop-compose.yml up -d" "å¯åŠ¨Dockerå®¹å™¨"
if [ $? -eq 0 ]; then
    log_success "å®¹å™¨å¯åŠ¨å‘½ä»¤æ‰§è¡ŒæˆåŠŸ"
    log_info "ç­‰å¾…å®¹å™¨å®Œå…¨å¯åŠ¨..."
    sleep 10
else
    log_error "å®¹å™¨å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥docker-compose.ymlæ–‡ä»¶"
    exit 1
fi

# æ£€æŸ¥å¿…è¦çš„å®¹å™¨æ˜¯å¦è¿è¡Œ
log_step "1" "æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€"
containers=("hadoop-master1" "hadoop-master2" "hadoop-master3" "hadoop-worker1" "hadoop-worker2" "hadoop-worker3")
for container in "${containers[@]}"; do
    if check_container "$container"; then
        log_success "å®¹å™¨ $container æ­£åœ¨è¿è¡Œ"
    else
        log_error "å®¹å™¨ $container æœªè¿è¡Œï¼Œå¯åŠ¨å¯èƒ½å¤±è´¥"
        exit 1
    fi
done

# SSH é…ç½®æ£€æŸ¥
log_step "2" "æ£€æŸ¥SSHå…å¯†ç™»å½•é…ç½®"
log_info "é…ç½®master1åˆ°å…¶ä»–èŠ‚ç‚¹çš„SSHè¿æ¥..."
docker exec hadoop-master1 ssh -o StrictHostKeyChecking=no hadoop-master2 exit
if [ $? -eq 0 ]; then
    log_success "master1 -> master2 SSHè¿æ¥æˆåŠŸ"
else
    log_warning "master1 -> master2 SSHè¿æ¥å¤±è´¥"
fi

docker exec hadoop-master1 ssh -o StrictHostKeyChecking=no hadoop-master3 exit
if [ $? -eq 0 ]; then
    log_success "master1 -> master3 SSHè¿æ¥æˆåŠŸ"
else
    log_warning "master1 -> master3 SSHè¿æ¥å¤±è´¥"
fi

log_info "é…ç½®master2åˆ°å…¶ä»–èŠ‚ç‚¹çš„SSHè¿æ¥..."
docker exec hadoop-master2 ssh -o StrictHostKeyChecking=no hadoop-master1 exit
docker exec hadoop-master2 ssh -o StrictHostKeyChecking=no hadoop-master3 exit

log_info "é…ç½®master3åˆ°å…¶ä»–èŠ‚ç‚¹çš„SSHè¿æ¥..."
docker exec hadoop-master3 ssh -o StrictHostKeyChecking=no hadoop-master1 exit
docker exec hadoop-master3 ssh -o StrictHostKeyChecking=no hadoop-master2 exit

log_success "SSHå…å¯†ç™»å½•é…ç½®å®Œæˆ"

# å¯åŠ¨ journalnode
log_step "3" "å¯åŠ¨JournalNodeæœåŠ¡"
log_info "åœ¨MasterèŠ‚ç‚¹å¯åŠ¨JournalNode..."
docker exec hadoop-master1 /opt/hadoop/bin/hdfs --daemon start journalnode
docker exec hadoop-master2 /opt/hadoop/bin/hdfs --daemon start journalnode
docker exec hadoop-master3 /opt/hadoop/bin/hdfs --daemon start journalnode

log_info "åœ¨WorkerèŠ‚ç‚¹å¯åŠ¨JournalNodeï¼ˆå¯é€‰ï¼‰..."
# å¯ä»¥ä¸å¯åŠ¨ worker èŠ‚ç‚¹ä¸Šçš„ journalnode
docker exec hadoop-worker1 /opt/hadoop/bin/hdfs --daemon start journalnode
docker exec hadoop-worker2 /opt/hadoop/bin/hdfs --daemon start journalnode
docker exec hadoop-worker3 /opt/hadoop/bin/hdfs --daemon start journalnode

log_success "JournalNodeæœåŠ¡å¯åŠ¨å®Œæˆ"
sleep 3

# åˆå§‹åŒ– NameNode
log_step "4" "åˆå§‹åŒ–ä¸»NameNode"
log_info "æ ¼å¼åŒ–master1ä¸Šçš„NameNode..."
docker exec hadoop-master1 bash /opt/hadoop/bin/hdfs namenode -format -force
if [ $? -eq 0 ]; then
    log_success "NameNodeæ ¼å¼åŒ–æˆåŠŸ"
else
    log_error "NameNodeæ ¼å¼åŒ–å¤±è´¥"
    exit 1
fi

log_info "å¯åŠ¨master1ä¸Šçš„NameNode..."
docker exec hadoop-master1 /opt/hadoop/bin/hdfs --daemon start namenode
sleep 5

# Bootstrap Standby
log_step "5" "é…ç½®å¤‡ç”¨NameNode"
log_info "é…ç½®master2ä½œä¸ºStandby NameNode..."
docker exec -it hadoop-master2 bash /opt/hadoop/bin/hdfs namenode -bootstrapStandby -force
if [ $? -eq 0 ]; then
    log_success "master2 Standby NameNodeé…ç½®æˆåŠŸ"
else
    log_error "master2 Standby NameNodeé…ç½®å¤±è´¥"
fi

docker exec hadoop-master2 /opt/hadoop/bin/hdfs --daemon start namenode

log_info "é…ç½®master3ä½œä¸ºStandby NameNode..."
docker exec -it hadoop-master3 bash /opt/hadoop/bin/hdfs namenode -bootstrapStandby -force
if [ $? -eq 0 ]; then
    log_success "master3 Standby NameNodeé…ç½®æˆåŠŸ"
else
    log_error "master3 Standby NameNodeé…ç½®å¤±è´¥"
fi

docker exec hadoop-master3 /opt/hadoop/bin/hdfs --daemon start namenode

log_success "å¤‡ç”¨NameNodeé…ç½®å®Œæˆ"
sleep 3

# åœæ­¢ DFS
log_step "6" "åœæ­¢DFSæœåŠ¡å‡†å¤‡é‡æ–°é…ç½®"
log_info "åœæ­¢åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ..."
docker exec hadoop-master1 /opt/hadoop/sbin/stop-dfs.sh
sleep 5

# Zookeeper æ•°æ®é‡æ–°æ ¼å¼åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
log_step "7" "åˆå§‹åŒ–ZooKeeperæ•…éšœåˆ‡æ¢æ§åˆ¶å™¨"
log_info "æ ¼å¼åŒ–ZooKeeperä¸­çš„HAçŠ¶æ€ä¿¡æ¯..."
docker exec -it hadoop-master1 bash /opt/hadoop/bin/hdfs zkfc -formatZK -force
if [ $? -eq 0 ]; then
    log_success "ZooKeeperæ ¼å¼åŒ–æˆåŠŸ"
else
    log_error "ZooKeeperæ ¼å¼åŒ–å¤±è´¥"
fi

# å¯åŠ¨ zkfc å’Œ DFS/YARN
log_step "8" "å¯åŠ¨HadoopæœåŠ¡"
log_info "å¯åŠ¨ZooKeeperæ•…éšœåˆ‡æ¢æ§åˆ¶å™¨..."
docker exec hadoop-master1 /opt/hadoop/bin/hdfs --daemon start zkfc

log_info "å¯åŠ¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ..."
docker exec hadoop-master1 /opt/hadoop/sbin/start-dfs.sh
sleep 5

log_info "å¯åŠ¨YARNèµ„æºç®¡ç†å™¨..."
docker exec hadoop-master1 /opt/hadoop/sbin/start-yarn.sh
sleep 5

log_success "HadoopæœåŠ¡å¯åŠ¨å®Œæˆ"

log_step "9" "éªŒè¯æœåŠ¡çŠ¶æ€"
log_info "æ£€æŸ¥NameNodeçŠ¶æ€..."
exec_and_log "docker exec hadoop-master1 /opt/hadoop/bin/hdfs haadmin -getServiceState nn1" "æ£€æŸ¥NameNode nn1çŠ¶æ€"
exec_and_log "docker exec hadoop-master1 /opt/hadoop/bin/hdfs haadmin -getServiceState nn2" "æ£€æŸ¥NameNode nn2çŠ¶æ€"
exec_and_log "docker exec hadoop-master1 /opt/hadoop/bin/hdfs dfsadmin -report" "æ£€æŸ¥HDFSé›†ç¾¤çŠ¶æ€"

echo -e "\n${GREEN}========================================${NC}"
echo -e "${GREEN}  Hadoop HAé›†ç¾¤åˆå§‹åŒ–å®Œæˆï¼${NC}"
echo -e "${GREEN}========================================${NC}"
echo -e "${YELLOW}è®¿é—®ä¿¡æ¯ï¼š${NC}"
echo -e "  â€¢ NameNode Web UI: http://hadoop-master1:9870"
echo -e "  â€¢ ResourceManager Web UI: http://hadoop-master1:8088"
echo -e "  â€¢ DataNode Web UI: http://hadoop-worker1:9864"
echo -e "${YELLOW}å¸¸ç”¨å‘½ä»¤ï¼š${NC}"
echo -e "  â€¢ æ£€æŸ¥é›†ç¾¤çŠ¶æ€: docker exec hadoop-master1 /opt/hadoop/bin/hdfs dfsadmin -report"
echo -e "  â€¢ æ£€æŸ¥HAçŠ¶æ€: docker exec hadoop-master1 /opt/hadoop/bin/hdfs haadmin -getServiceState nn1"
echo -e "${YELLOW}å¦‚éœ€åœæ­¢æœåŠ¡ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š${NC}"

# è®°å½•å®Œæˆæ—¥å¿—
log_success "Hadoop é«˜å¯ç”¨é›†ç¾¤åˆå§‹åŒ–è„šæœ¬æ‰§è¡Œå®Œæˆï¼"
log_info "å®Œæˆæ—¶é—´: $(date '+%Y-%m-%d %H:%M:%S')"
log_info "æ—¥å¿—å·²ä¿å­˜åˆ°: $LOG_FILE"
echo "" >> "$LOG_FILE"
echo "=====================================" >> "$LOG_FILE"
echo "è„šæœ¬æ‰§è¡Œå®Œæˆ" >> "$LOG_FILE"
echo "ç»“æŸæ—¶é—´: $(date '+%Y-%m-%d %H:%M:%S')" >> "$LOG_FILE"
echo "=====================================" >> "$LOG_FILE"

# åœæ­¢æœåŠ¡çš„å‘½ä»¤ï¼ˆæ³¨é‡Šæ‰ï¼Œä¾›ç”¨æˆ·å‚è€ƒï¼‰
# docker exec hadoop-master1 /opt/hadoop/sbin/stop-yarn.sh
# docker exec hadoop-master1 /opt/hadoop/sbin/stop-dfs.sh
# docker exec hadoop-master1 /opt/hadoop/bin/hdfs --daemon stop zkfc
```

#### é›†ç¾¤ç®¡ç†è„šæœ¬

#### å¯åŠ¨è„šæœ¬(start-hadoop.sh)

> [!NOTE]
> æ–‡ä»¶å: start-hadoop.sh

```bash
#!/bin/bash
# start-hadoop.sh - Hadoopé›†ç¾¤å¯åŠ¨è„šæœ¬
# ä¼˜åŒ–ç‰ˆæœ¬ï¼šæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
BOLD='\033[1m'
NC='\033[0m' # No Color

# é›†ç¾¤èŠ‚ç‚¹é…ç½®
MASTER_NODES=("hadoop-master1" "hadoop-master2" "hadoop-master3")
WORKER_NODES=("hadoop-worker1" "hadoop-worker2" "hadoop-worker3")
ALL_NODES=("${MASTER_NODES[@]}" "${WORKER_NODES[@]}")

# æœåŠ¡é…ç½®
HADOOP_BIN="/opt/hadoop/bin"
HADOOP_SBIN="/opt/hadoop/sbin"
STARTUP_TIMEOUT=120
HEALTH_CHECK_TIMEOUT=30

# æ‰“å°å¸¦é¢œè‰²çš„æ¶ˆæ¯
print_message() {
    local color=$1
    local message=$2
    echo -e "${color}${message}${NC}"
}

# æ‰“å°æˆåŠŸæ¶ˆæ¯
print_success() {
    print_message $GREEN "âœ“ $1"
}

# æ‰“å°è­¦å‘Šæ¶ˆæ¯
print_warning() {
    print_message $YELLOW "âš  $1"
}

# æ‰“å°é”™è¯¯æ¶ˆæ¯
print_error() {
    print_message $RED "âœ— $1"
}

# æ‰“å°ä¿¡æ¯æ¶ˆæ¯
print_info() {
    print_message $BLUE "â„¹ $1"
}

# æ‰“å°æ­¥éª¤æ ‡é¢˜
print_step() {
    print_message $PURPLE "${BOLD}=== $1 ===${NC}"
}

# æ˜¾ç¤ºå¯åŠ¨åŠ¨ç”»
show_progress() {
    local duration=$1
    local description=$2
    local chars="â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â "
    local delay=0.1
    local i=0
    
    while [ $i -lt $duration ]; do
        for (( j=0; j<${#chars}; j++ )); do
            printf "\r${BLUE}${chars:$j:1} $description... ${CYAN}(%ds)${NC}" $((duration - i))
            sleep $delay
            i=$((i + 1))
            if [ $i -ge $duration ]; then
                break
            fi
        done
    done
    printf "\r${GREEN}âœ“ $description å®Œæˆ${NC}\n"
}

# æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€
check_containers() {
    print_step "æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€"
    
    local failed_containers=()
    
    for node in "${ALL_NODES[@]}"; do
        if docker ps | grep -q "$node"; then
            print_success "$node å®¹å™¨æ­£åœ¨è¿è¡Œ"
        else
            print_error "$node å®¹å™¨æœªè¿è¡Œ"
            failed_containers+=("$node")
        fi
    done
    
    if [ ${#failed_containers[@]} -gt 0 ]; then
        print_error "ä»¥ä¸‹å®¹å™¨æœªè¿è¡Œ: ${failed_containers[*]}"
        print_info "è¯·å…ˆå¯åŠ¨æ‰€æœ‰Hadoopå®¹å™¨"
        return 1
    fi
    
    print_success "æ‰€æœ‰å®¹å™¨çŠ¶æ€æ­£å¸¸"
    return 0
}

# æ‰§è¡Œå‘½ä»¤å¹¶æ£€æŸ¥ç»“æœ
execute_command() {
    local node=$1
    local command=$2
    local description=$3
    
    if docker exec "$node" $command &>/dev/null; then
        print_success "$node: $description"
        return 0
    else
        print_error "$node: $description å¤±è´¥"
        return 1
    fi
}

# å¯åŠ¨JournalNodeæœåŠ¡
start_journalnodes() {
    print_step "å¯åŠ¨JournalNodeæœåŠ¡"
    
    local failed_nodes=()
    
    for node in "${ALL_NODES[@]}"; do
        if execute_command "$node" "$HADOOP_BIN/hdfs --daemon start journalnode" "å¯åŠ¨JournalNode"; then
            continue
        else
            failed_nodes+=("$node")
        fi
    done
    
    if [ ${#failed_nodes[@]} -gt 0 ]; then
        print_warning "ä»¥ä¸‹èŠ‚ç‚¹JournalNodeå¯åŠ¨å¤±è´¥: ${failed_nodes[*]}"
    fi
    
    # ç­‰å¾…JournalNodeå¯åŠ¨
    show_progress 5 "ç­‰å¾…JournalNodeæœåŠ¡å¯åŠ¨"
    
    return 0
}

# å¯åŠ¨ZKFCæœåŠ¡
start_zkfc() {
    print_step "å¯åŠ¨ZKFCæœåŠ¡"
    
    execute_command "hadoop-master1" "$HADOOP_BIN/hdfs --daemon start zkfc" "å¯åŠ¨ZKFC"
    
    # ç­‰å¾…ZKFCå¯åŠ¨
    show_progress 3 "ç­‰å¾…ZKFCæœåŠ¡å¯åŠ¨"
    
    return 0
}

# å¯åŠ¨HDFSæœåŠ¡
start_hdfs() {
    print_step "å¯åŠ¨HDFSæœåŠ¡"
    
    print_info "ä» hadoop-master1 å¯åŠ¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ..."
    
    if docker exec hadoop-master1 $HADOOP_SBIN/start-dfs.sh; then
        print_success "HDFSå¯åŠ¨å‘½ä»¤æ‰§è¡ŒæˆåŠŸ"
    else
        print_error "HDFSå¯åŠ¨å¤±è´¥"
        return 1
    fi
    
    # ç­‰å¾…HDFSæœåŠ¡å¯åŠ¨
    show_progress 10 "ç­‰å¾…HDFSæœåŠ¡å®Œå…¨å¯åŠ¨"
    
    return 0
}

# å¯åŠ¨YARNæœåŠ¡
start_yarn() {
    print_step "å¯åŠ¨YARNæœåŠ¡"
    
    print_info "ä» hadoop-master1 å¯åŠ¨èµ„æºç®¡ç†å™¨..."
    
    if docker exec hadoop-master1 $HADOOP_SBIN/start-yarn.sh; then
        print_success "YARNå¯åŠ¨å‘½ä»¤æ‰§è¡ŒæˆåŠŸ"
    else
        print_error "YARNå¯åŠ¨å¤±è´¥"
        return 1
    fi
    
    # ç­‰å¾…YARNæœåŠ¡å¯åŠ¨
    show_progress 8 "ç­‰å¾…YARNæœåŠ¡å®Œå…¨å¯åŠ¨"
    
    return 0
}

# å¯åŠ¨JobHistory Server
start_jobhistory() {
    print_step "å¯åŠ¨JobHistory Server"
    
    local failed_nodes=()
    
    for node in "${MASTER_NODES[@]}"; do
        if execute_command "$node" "$HADOOP_BIN/mapred --daemon start historyserver" "å¯åŠ¨JobHistory Server"; then
            continue
        else
            failed_nodes+=("$node")
        fi
    done
    
    if [ ${#failed_nodes[@]} -gt 0 ]; then
        print_warning "ä»¥ä¸‹èŠ‚ç‚¹JobHistory Serverå¯åŠ¨å¤±è´¥: ${failed_nodes[*]}"
    fi
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    show_progress 5 "ç­‰å¾…JobHistory Serverå¯åŠ¨"
    
    return 0
}

# å¥åº·æ£€æŸ¥
health_check() {
    print_step "é›†ç¾¤å¥åº·æ£€æŸ¥"
    
    local checks=(
        "check_hdfs_namenode:æ£€æŸ¥HDFS NameNode"
        "check_yarn_resourcemanager:æ£€æŸ¥YARN ResourceManager"
        "check_cluster_nodes:æ£€æŸ¥é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€"
    )
    
    for check_info in "${checks[@]}"; do
        local func_name="${check_info%%:*}"
        local description="${check_info##*:}"
        
        print_info "$description..."
        if $func_name; then
            print_success "$description é€šè¿‡"
        else
            print_warning "$description å¤±è´¥æˆ–å¼‚å¸¸"
        fi
    done
}

# æ£€æŸ¥HDFS NameNodeçŠ¶æ€
check_hdfs_namenode() {
    docker exec hadoop-master1 $HADOOP_BIN/hdfs dfs -ls / &>/dev/null
}

# æ£€æŸ¥YARN ResourceManagerçŠ¶æ€
check_yarn_resourcemanager() {
    docker exec hadoop-master1 $HADOOP_BIN/yarn node -list &>/dev/null
}

# æ£€æŸ¥é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€
check_cluster_nodes() {
    local active_nodes=$(docker exec hadoop-master1 $HADOOP_BIN/hdfs dfsadmin -report 2>/dev/null | grep "Live datanodes" | cut -d'(' -f2 | cut -d')' -f1 || echo "0")
    if [ "$active_nodes" -gt 0 ]; then
        print_info "å‘ç° $active_nodes ä¸ªæ´»è·ƒçš„DataNode"
        return 0
    else
        return 1
    fi
}

# æ˜¾ç¤ºé›†ç¾¤çŠ¶æ€
show_cluster_status() {
    print_step "é›†ç¾¤çŠ¶æ€æ¦‚è§ˆ"
    
    echo
    print_message $CYAN "HDFSçŠ¶æ€:"
    docker exec hadoop-master1 $HADOOP_BIN/hdfs dfsadmin -report 2>/dev/null | head -10 || print_warning "æ— æ³•è·å–HDFSçŠ¶æ€"
    
    echo
    print_message $CYAN "YARNèŠ‚ç‚¹çŠ¶æ€:"
    docker exec hadoop-master1 $HADOOP_BIN/yarn node -list 2>/dev/null || print_warning "æ— æ³•è·å–YARNèŠ‚ç‚¹çŠ¶æ€"
    
    echo
    print_message $CYAN "Webè®¿é—®åœ°å€:"
    echo "  - HDFS NameNode: http://localhost:9870"
    echo "  - YARN ResourceManager: http://localhost:8088"
    echo "  - JobHistory Server: http://localhost:19888"
}

# æ˜¾ç¤ºå¯åŠ¨æ‘˜è¦
show_startup_summary() {
    local start_time=$1
    local end_time=$2
    local duration=$((end_time - start_time))
    
    print_step "å¯åŠ¨æ‘˜è¦"
    
    print_message $GREEN "=========================================="
    print_message $GREEN "       Hadoopé›†ç¾¤å¯åŠ¨å®Œæˆï¼"
    print_message $GREEN "=========================================="
    echo
    print_success "å¯åŠ¨è€—æ—¶: ${duration}ç§’"
    print_success "é›†ç¾¤èŠ‚ç‚¹: ${#ALL_NODES[@]} ä¸ª"
    print_success "ä¸»èŠ‚ç‚¹: ${#MASTER_NODES[@]} ä¸ª"
    print_success "å·¥ä½œèŠ‚ç‚¹: ${#WORKER_NODES[@]} ä¸ª"
}

# ä¸»å‡½æ•°
main() {
    local start_time=$(date +%s)
    
    # æ˜¾ç¤ºæ ‡é¢˜
    echo
    print_message $PURPLE "${BOLD}========================================"
    print_message $PURPLE "${BOLD}    Hadoopé›†ç¾¤å¯åŠ¨è„šæœ¬ (ä¼˜åŒ–ç‰ˆ)"
    print_message $PURPLE "${BOLD}========================================"
    echo
    
    # æ˜¾ç¤ºé›†ç¾¤é…ç½®
    print_message $CYAN "é›†ç¾¤é…ç½®ä¿¡æ¯:"
    echo "  - ä¸»èŠ‚ç‚¹: ${MASTER_NODES[*]}"
    echo "  - å·¥ä½œèŠ‚ç‚¹: ${WORKER_NODES[*]}"
    echo "  - å¯åŠ¨è¶…æ—¶: ${STARTUP_TIMEOUT}ç§’"
    echo
    
    # æ‰§è¡Œå¯åŠ¨æ­¥éª¤
    local steps=(
        "check_containers:æ£€æŸ¥å®¹å™¨çŠ¶æ€"
        "start_journalnodes:å¯åŠ¨JournalNode"
        "start_zkfc:å¯åŠ¨ZKFC"
        "start_hdfs:å¯åŠ¨HDFS"
        "start_yarn:å¯åŠ¨YARN"
        "start_jobhistory:å¯åŠ¨JobHistory Server"
        "health_check:å¥åº·æ£€æŸ¥"
    )
    
    local total_steps=${#steps[@]}
    local current_step=0
    local failed_steps=()
    
    for step_info in "${steps[@]}"; do
        current_step=$((current_step + 1))
        local func_name="${step_info%%:*}"
        local description="${step_info##*:}"
        
        print_message $BLUE "[$current_step/$total_steps] $description"
        echo
        
        if ! $func_name; then
            failed_steps+=("$description")
            print_error "æ­¥éª¤å¤±è´¥: $description"
            
            # å¯¹äºå…³é”®æ­¥éª¤å¤±è´¥ï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
            if [[ "$func_name" == "check_containers" ]]; then
                print_message $YELLOW "å…³é”®æ­¥éª¤å¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ[y/N]"
                read -t 10 -n 1 -r
                echo
                if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                    print_error "ç”¨æˆ·é€‰æ‹©é€€å‡º"
                    exit 1
                fi
            fi
        fi
        echo
    done
    
    local end_time=$(date +%s)
    
    # æ˜¾ç¤ºå¯åŠ¨ç»“æœ
    if [ ${#failed_steps[@]} -eq 0 ]; then
        show_startup_summary $start_time $end_time
    else
        print_message $YELLOW "========================================"
        print_message $YELLOW "    Hadoopé›†ç¾¤å¯åŠ¨å®Œæˆï¼ˆæœ‰è­¦å‘Šï¼‰"
        print_message $YELLOW "========================================"
        echo
        print_warning "ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œå¤±è´¥æˆ–æœ‰è­¦å‘Š:"
        for failed_step in "${failed_steps[@]}"; do
            print_error "- $failed_step"
        done
        echo
        print_info "å¯åŠ¨è€—æ—¶: $((end_time - start_time))ç§’"
    fi
    
    # è¯¢é—®æ˜¯å¦æ˜¾ç¤ºé›†ç¾¤çŠ¶æ€
    echo
    print_info "æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„é›†ç¾¤çŠ¶æ€ï¼Ÿ[y/N]"
    read -t 10 -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        show_cluster_status
    fi
    
    echo
    print_message $CYAN "æç¤º: å¦‚éœ€åœæ­¢é›†ç¾¤ï¼Œè¯·è¿è¡Œç›¸åº”çš„åœæ­¢è„šæœ¬"
    print_message $CYAN "é›†ç¾¤ç°åœ¨å¯ä»¥æ¥å—ä½œä¸šæäº¤"
}

# é”™è¯¯å¤„ç†
set -e
trap 'print_error "è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä½ç½®: $BASH_COMMAND"' ERR

# å…è®¸è„šæœ¬åœ¨æŸäº›å‘½ä»¤å¤±è´¥æ—¶ç»§ç»­è¿è¡Œ
set +e

# è¿è¡Œä¸»å‡½æ•°
main "$@"
```

#### åœæ­¢è„šæœ¬(stop-hadoop.sh)

> [!NOTE]
> æ–‡ä»¶å: stop-hadoop.sh

```bash
#!/bin/bash
echo "=== åœæ­¢Hadoopé›†ç¾¤ ==="

echo "åœæ­¢JobHistory Server..."
docker exec hadoop-master3 /opt/hadoop/bin/mapred --daemon stop historyserver
docker exec hadoop-master2 /opt/hadoop/bin/mapred --daemon stop historyserver
docker exec hadoop-master1 /opt/hadoop/bin/mapred --daemon stop historyserver

echo "åœæ­¢YARN..."
docker exec hadoop-master1 /opt/hadoop/sbin/stop-yarn.sh

sleep 5

echo "åœæ­¢HDFS..."
docker exec hadoop-master1 /opt/hadoop/sbin/stop-dfs.sh

echo "åœæ­¢ZKFC..."
docker exec hadoop-master1 /opt/hadoop/bin/hdfs --daemon stop zkfc

sleep 5

echo "åœæ­¢JournalNode..."
docker exec hadoop-worker3 /opt/hadoop/bin/hdfs --daemon stop journalnode
docker exec hadoop-worker2 /opt/hadoop/bin/hdfs --daemon stop journalnode
docker exec hadoop-worker1 /opt/hadoop/bin/hdfs --daemon stop journalnode
docker exec hadoop-master3 /opt/hadoop/bin/hdfs --daemon stop journalnode
docker exec hadoop-master2 /opt/hadoop/bin/hdfs --daemon stop journalnode
docker exec hadoop-master1 /opt/hadoop/bin/hdfs --daemon stop journalnode

echo "=== Hadoopé›†ç¾¤åœæ­¢å®Œæˆ ==="
```

### æ­¥éª¤ 5ï¼šéªŒè¯ Hadoop é›†ç¾¤

#### æ£€æŸ¥æœåŠ¡çŠ¶æ€

> [!NOTE]
> æ–‡ä»¶å: check-hadoop.sh

```bash
#!/bin/bash

echo "=== Hadoopé›†ç¾¤çŠ¶æ€æ£€æŸ¥ ==="

echo "--- hadoop-master1 è¿›ç¨‹ ---"
docker exec hadoop-master1 jps

echo "--- hadoop-master2 è¿›ç¨‹ ---"
docker exec hadoop-master2 jps

echo "--- hadoop-master3 è¿›ç¨‹ ---"
docker exec hadoop-master3 jps

echo "--- hadoop-worker1 è¿›ç¨‹ ---"
docker exec hadoop-worker1 jps

echo "--- hadoop-worker2 è¿›ç¨‹ ---"
docker exec hadoop-worker2 jps

echo "--- hadoop-worker3 è¿›ç¨‹ ---"
docker exec hadoop-worker3 jps

echo "--- HDFSé›†ç¾¤æŠ¥å‘Š ---"
docker exec hadoop-master1 /opt/hadoop/bin/hdfs dfsadmin -report

echo "--- YARNèŠ‚ç‚¹åˆ—è¡¨ ---"
docker exec hadoop-master1 /opt/hadoop/bin/yarn node -list
```

#### Web ç•Œé¢è®¿é—®

- **NameNode Web UI**: http://hadoop-master1:9870
- **ResourceManager Web UI**: http://hadoop-master1:8088

## é˜¶æ®µäºŒï¼šHBase åˆ†å¸ƒå¼æ•°æ®åº“

HBase æ˜¯åŸºäº Hadoop çš„ NoSQL æ•°æ®åº“ï¼Œæä¾›å®æ—¶è¯»å†™èƒ½åŠ›å’Œå¤§è§„æ¨¡æ•°æ®å­˜å‚¨ã€‚

### HBase æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "HBase Masterå±‚"
        HM1[HMaster-1<br/>Active]
        HM2[HMaster-2<br/>Backup]
        HM3[HMaster-3<br/>Backup]
    end

    subgraph "HBase RegionServerå±‚"
        RS1[RegionServer-1<br/>hadoop-master1]
        RS2[RegionServer-2<br/>hadoop-master2]
        RS3[RegionServer-3<br/>hadoop-master3]
    end

    subgraph "å­˜å‚¨å±‚"
        HDFS[HDFSåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ<br/>mycluster]
    end

    subgraph "åè°ƒå±‚"
        ZK[ZooKeeperé›†ç¾¤<br/>zoo1,zoo2,zoo3]
    end

    subgraph "æ•°æ®ç»„ç»‡"
        T1[Table 1]
        T2[Table 2]
        R1[Region 1]
        R2[Region 2]
        R3[Region 3]

        T1 --> R1
        T1 --> R2
        T2 --> R3
    end

    HM1 --> RS1
    HM1 --> RS2
    HM1 --> RS3

    RS1 --> HDFS
    RS2 --> HDFS
    RS3 --> HDFS

    HM1 -.-> ZK
    HM2 -.-> ZK
    HM3 -.-> ZK

    RS1 -.-> ZK
    RS2 -.-> ZK
    RS3 -.-> ZK

    RS1 --> R1
    RS2 --> R2
    RS3 --> R3

    style HM1 fill:#ffcdd2
    style HM2 fill:#e0e0e0
    style HM3 fill:#e0e0e0
    style RS1 fill:#e1f5fe
    style RS2 fill:#e1f5fe
    style RS3 fill:#e1f5fe
    style HDFS fill:#fff3e0
    style ZK fill:#e8f5e8
```

### æ­¥éª¤ 1ï¼šHBase é…ç½®

#### HBase ç¯å¢ƒé…ç½®

#### hbase-env.sh

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/hbase/conf/hbase-env.sh

```bash
# Javaç¯å¢ƒ
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

# HBaseç®¡ç†ZooKeeperï¼ˆè®¾ä¸ºfalseä½¿ç”¨å¤–éƒ¨ZKï¼‰
export HBASE_MANAGES_ZK=false

# ç¦ç”¨Hadoop classpathæŸ¥æ‰¾ï¼Œé¿å…ç±»è·¯å¾„å†²çª
export HBASE_DISABLE_HADOOP_CLASSPATH_LOOKUP="true"

# HBaseå †å†…å­˜è®¾ç½®
export HBASE_HEAPSIZE=2G

# RegionServerå †å†…å­˜è®¾ç½®
export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Xmx2048m"

# Masterå †å†…å­˜è®¾ç½®
export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Xmx1024m"

# åƒåœ¾å›æ”¶é…ç½®
export HBASE_OPTS="$HBASE_OPTS -XX:+UseG1GC -XX:+UnlockExperimentalVMOptions"
```

#### hbase-site.xml

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/hbase/conf/hbase-site.xml

```xml
<?xml version="1.0"?>
<configuration>
    <!-- é›†ç¾¤æ¨¡å¼é…ç½® -->
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>

    <!-- HBaseæ ¹ç›®å½•ï¼ŒæŒ‡å‘HDFSé«˜å¯ç”¨å‘½åæœåŠ¡ -->
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://mycluster/hbase</value>
    </property>

    <!-- ä¸´æ—¶ç›®å½• -->
    <property>
        <name>hbase.tmp.dir</name>
        <value>./tmp</value>
    </property>

    <!-- å®‰å…¨é…ç½® -->
    <property>
        <name>hbase.unsafe.stream.capability.enforce</name>
        <value>false</value>
    </property>

    <!-- ZooKeeperé…ç½® -->
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>zoo1,zoo2,zoo3</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.dataDir</name>
        <value>/data/zookeeper</value>
    </property>

    <!-- Masteré«˜å¯ç”¨é…ç½® -->
    <property>
        <name>hbase.master.wait.on.zk</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.master.znode</name>
        <value>/hbase/master</value>
    </property>

    <!-- RegionServeré…ç½® -->
    <property>
        <name>hbase.regionserver.handler.count</name>
        <value>30</value>
    </property>
    <property>
        <name>hbase.regionserver.port</name>
        <value>16020</value>
    </property>

    <!-- åˆ†åŒºåˆ†å‰²ç­–ç•¥ -->
    <property>
        <name>hbase.hregion.max.filesize</name>
        <value>10737418240</value> <!-- 10GB -->
    </property>

    <!-- WALé…ç½® -->
    <property>
        <name>hbase.wal.provider</name>
        <value>filesystem</value>
    </property>

    <!-- å®¢æˆ·ç«¯é…ç½® -->
    <property>
        <name>hbase.client.write.buffer</name>
        <value>20971520</value> <!-- 20MB -->
    </property>
</configuration>
```

#### RegionServer å’Œå¤‡ç”¨ Master é…ç½®

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/hbase/conf/regionservers

```bash
hadoop-master1
hadoop-master2
hadoop-master3
```

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/hbase/conf/backup-masters

```bash
hadoop-master2
hadoop-master3
```

#### å¤åˆ¶ Hadoop é…ç½®

```bash
# å¤åˆ¶Hadoopé…ç½®æ–‡ä»¶åˆ°HBase
cp ~/opt/docker-data/hadoop-hbase-spark/hadoop/etc/hadoop/core-site.xml ~/opt/docker-data/hadoop-hbase-spark/hbase/conf/
cp ~/opt/docker-data/hadoop-hbase-spark/hadoop/etc/hadoop/hdfs-site.xml ~/opt/docker-data/hadoop-hbase-spark/hbase/conf/
```

### æ­¥éª¤ 2ï¼šå¯åŠ¨ HBase

#### HBase å¯åŠ¨è„šæœ¬

> [!NOTE]
> æ–‡ä»¶å: start-hbase.sh

```bash
#!/bin/bash
# start-hbase.sh - HBaseé›†ç¾¤å¯åŠ¨è„šæœ¬
# ä¼˜åŒ–ç‰ˆæœ¬ï¼šæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
BOLD='\033[1m'
NC='\033[0m' # No Color

# é…ç½®å˜é‡
MASTER_CONTAINER="hadoop-master1"
HBASE_BIN="/opt/hbase/bin"
HADOOP_BIN="/opt/hadoop/bin"
HBASE_DIR="/hbase"
STARTUP_TIMEOUT=120
HEALTH_CHECK_RETRIES=5

# æ‰“å°å¸¦é¢œè‰²çš„æ¶ˆæ¯
print_message() {
    local color=$1
    local message=$2
    echo -e "${color}${message}${NC}"
}

# æ‰“å°æˆåŠŸæ¶ˆæ¯
print_success() {
    print_message $GREEN "âœ“ $1"
}

# æ‰“å°è­¦å‘Šæ¶ˆæ¯
print_warning() {
    print_message $YELLOW "âš  $1"
}

# æ‰“å°é”™è¯¯æ¶ˆæ¯
print_error() {
    print_message $RED "âœ— $1"
}

# æ‰“å°ä¿¡æ¯æ¶ˆæ¯
print_info() {
    print_message $BLUE "â„¹ $1"
}

# æ‰“å°æ­¥éª¤æ ‡é¢˜
print_step() {
    print_message $PURPLE "${BOLD}=== $1 ===${NC}"
}

# æ˜¾ç¤ºå¯åŠ¨åŠ¨ç”»
show_progress() {
    local duration=$1
    local description=$2
    local chars="â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â "
    local delay=0.1
    local i=0
    
    while [ $i -lt $duration ]; do
        for (( j=0; j<${#chars}; j++ )); do
            printf "\r${BLUE}${chars:$j:1} $description... ${CYAN}(%ds)${NC}" $((duration - i))
            sleep $delay
            i=$((i + 1))
            if [ $i -ge $duration ]; then
                break
            fi
        done
    done
    printf "\r${GREEN}âœ“ $description å®Œæˆ${NC}\n"
}

# æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€
check_container() {
    print_step "æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€"
    
    if ! command -v docker &> /dev/null; then
        print_error "Docker æœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­"
        return 1
    fi
    
    if ! docker ps | grep -q "$MASTER_CONTAINER"; then
        print_error "å®¹å™¨ '$MASTER_CONTAINER' æœªè¿è¡Œ"
        print_info "è¯·å…ˆå¯åŠ¨Hadoopé›†ç¾¤å®¹å™¨"
        return 1
    fi
    
    print_success "å®¹å™¨ '$MASTER_CONTAINER' æ­£åœ¨è¿è¡Œ"
    return 0
}

# æ£€æŸ¥Hadoopé›†ç¾¤çŠ¶æ€
check_hadoop_cluster() {
    print_step "æ£€æŸ¥Hadoopé›†ç¾¤çŠ¶æ€"
    
    # æ£€æŸ¥HDFSæ˜¯å¦å¯è®¿é—®
    print_info "æ£€æŸ¥HDFSæœåŠ¡..."
    if ! docker exec "$MASTER_CONTAINER" "$HADOOP_BIN/hdfs" dfs -ls / &>/dev/null; then
        print_error "HDFSæœåŠ¡ä¸å¯ç”¨"
        print_info "è¯·å…ˆå¯åŠ¨Hadoopé›†ç¾¤"
        return 1
    fi
    print_success "HDFSæœåŠ¡æ­£å¸¸"
    
    # æ£€æŸ¥HDFSå®‰å…¨æ¨¡å¼
    print_info "æ£€æŸ¥HDFSå®‰å…¨æ¨¡å¼..."
    local safemode_status=$(docker exec "$MASTER_CONTAINER" "$HADOOP_BIN/hdfs" dfsadmin -safemode get 2>/dev/null)
    
    if echo "$safemode_status" | grep -q "ON"; then
        print_warning "HDFSå¤„äºå®‰å…¨æ¨¡å¼ï¼Œç­‰å¾…é€€å‡º..."
        show_progress 10 "ç­‰å¾…HDFSé€€å‡ºå®‰å…¨æ¨¡å¼"
        
        # å°è¯•ç­‰å¾…å®‰å…¨æ¨¡å¼è‡ªåŠ¨é€€å‡º
        if docker exec "$MASTER_CONTAINER" timeout 60 "$HADOOP_BIN/hdfs" dfsadmin -safemode wait; then
            print_success "HDFSå·²é€€å‡ºå®‰å…¨æ¨¡å¼"
        else
            print_warning "HDFSå®‰å…¨æ¨¡å¼ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•å¼ºåˆ¶é€€å‡º"
            docker exec "$MASTER_CONTAINER" "$HADOOP_BIN/hdfs" dfsadmin -safemode leave
        fi
    else
        print_success "HDFSä¸åœ¨å®‰å…¨æ¨¡å¼"
    fi
    
    return 0
}

# æ£€æŸ¥HBaseç›®å½•
check_hbase_directory() {
    print_step "æ£€æŸ¥HBase HDFSç›®å½•"
    
    if docker exec "$MASTER_CONTAINER" "$HADOOP_BIN/hdfs" dfs -test -d "$HBASE_DIR" 2>/dev/null; then
        print_success "HBaseç›®å½• '$HBASE_DIR' å­˜åœ¨"
        
        # æ˜¾ç¤ºç›®å½•æƒé™
        local permissions=$(docker exec "$MASTER_CONTAINER" "$HADOOP_BIN/hdfs" dfs -ls / 2>/dev/null | grep "hbase" | awk '{print $1}')
        if [[ -n "$permissions" ]]; then
            print_info "ç›®å½•æƒé™: $permissions"
        fi
    else
        print_warning "HBaseç›®å½• '$HBASE_DIR' ä¸å­˜åœ¨"
        print_info "å»ºè®®å…ˆè¿è¡Œ hbase-init.sh åˆå§‹åŒ–HBaseç›®å½•"
        
        print_message $YELLOW "æ˜¯å¦è‡ªåŠ¨åˆ›å»ºHBaseç›®å½•ï¼Ÿ[y/N]"
        read -t 10 -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            print_info "æ­£åœ¨åˆ›å»ºHBaseç›®å½•..."
            if docker exec "$MASTER_CONTAINER" "$HADOOP_BIN/hdfs" dfs -mkdir -p "$HBASE_DIR" && \
               docker exec "$MASTER_CONTAINER" "$HADOOP_BIN/hdfs" dfs -chmod 755 "$HBASE_DIR"; then
                print_success "HBaseç›®å½•åˆ›å»ºæˆåŠŸ"
            else
                print_error "HBaseç›®å½•åˆ›å»ºå¤±è´¥"
                return 1
            fi
        fi
    fi
    
    return 0
}

# å¯åŠ¨HBaseé›†ç¾¤
start_hbase_cluster() {
    print_step "å¯åŠ¨HBaseé›†ç¾¤"
    
    print_info "æ­£åœ¨å¯åŠ¨HBaseæœåŠ¡..."
    
    if docker exec "$MASTER_CONTAINER" "$HBASE_BIN/start-hbase.sh"; then
        print_success "HBaseå¯åŠ¨å‘½ä»¤æ‰§è¡ŒæˆåŠŸ"
    else
        print_error "HBaseå¯åŠ¨å¤±è´¥"
        return 1
    fi
    
    # ç­‰å¾…HBaseæœåŠ¡å¯åŠ¨
    show_progress 30 "ç­‰å¾…HBaseæœåŠ¡å®Œå…¨å¯åŠ¨"
    
    return 0
}

# å¥åº·æ£€æŸ¥
health_check() {
    print_step "HBaseæœåŠ¡å¥åº·æ£€æŸ¥"
    
    local retry_count=0
    local max_retries=$HEALTH_CHECK_RETRIES
    
    while [ $retry_count -lt $max_retries ]; do
        print_info "å¥åº·æ£€æŸ¥ ($((retry_count + 1))/$max_retries)..."
        
        # æ£€æŸ¥HBaseçŠ¶æ€
        if check_hbase_status; then
            print_success "HBaseæœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡"
            return 0
        else
            retry_count=$((retry_count + 1))
            if [ $retry_count -lt $max_retries ]; then
                print_warning "å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œç­‰å¾…é‡è¯•..."
                sleep 10
            fi
        fi
    done
    
    print_error "HBaseæœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œå·²é‡è¯• $max_retries æ¬¡"
    return 1
}

# æ£€æŸ¥HBaseçŠ¶æ€
check_hbase_status() {
    # å°è¯•è¿æ¥HBase shellå¹¶è·å–çŠ¶æ€
    local status_output
    if status_output=$(docker exec "$MASTER_CONTAINER" bash -c "echo 'status' | $HBASE_BIN/hbase shell 2>/dev/null | tail -10"); then
        if echo "$status_output" | grep -q "servers"; then
            return 0
        fi
    fi
    return 1
}

# æ˜¾ç¤ºHBaseè¯¦ç»†çŠ¶æ€
show_hbase_status() {
    print_step "HBaseé›†ç¾¤çŠ¶æ€è¯¦æƒ…"
    
    echo
    print_message $CYAN "HBaseé›†ç¾¤çŠ¶æ€:"
    docker exec "$MASTER_CONTAINER" bash -c "echo 'status \"detailed\"' | $HBASE_BIN/hbase shell 2>/dev/null" || print_warning "æ— æ³•è·å–è¯¦ç»†çŠ¶æ€"
    
    echo
    print_message $CYAN "HBaseç‰ˆæœ¬ä¿¡æ¯:"
    docker exec "$MASTER_CONTAINER" bash -c "echo 'version' | $HBASE_BIN/hbase shell 2>/dev/null" || print_warning "æ— æ³•è·å–ç‰ˆæœ¬ä¿¡æ¯"
}

# æ˜¾ç¤ºè¿æ¥ä¿¡æ¯
show_connection_info() {
    print_step "è¿æ¥ä¿¡æ¯"
    
    echo
    print_message $CYAN "HBase Webè®¿é—®åœ°å€:"
    echo "  - HBase Master: http://hadoop-master1:16010"
    echo "  - HBase Region Server: http://hadoop-master2:16030"
    
    echo
    print_message $CYAN "HBase Shellè¿æ¥å‘½ä»¤:"
    echo "  docker exec -it $MASTER_CONTAINER $HBASE_BIN/hbase shell"
    
    echo
    print_message $CYAN "HBaseé…ç½®ä¿¡æ¯:"
    echo "  - ZooKeeperç«¯å£: 2181"
    echo "  - Masterç«¯å£: 16000"
    echo "  - Region Serverç«¯å£: 16020"
}

# æµ‹è¯•HBaseåŸºæœ¬åŠŸèƒ½
test_hbase_functionality() {
    print_info "æµ‹è¯•HBaseåŸºæœ¬åŠŸèƒ½..."
    
    local test_table="test_table_$(date +%s)"
    local test_commands="
create '$test_table', 'cf'
put '$test_table', 'row1', 'cf:col1', 'value1'
get '$test_table', 'row1'
scan '$test_table'
disable '$test_table'
drop '$test_table'
"
    
    if docker exec "$MASTER_CONTAINER" bash -c "echo \"$test_commands\" | $HBASE_BIN/hbase shell 2>/dev/null" >/dev/null; then
        print_success "HBaseåŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡"
        return 0
    else
        print_warning "HBaseåŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥"
        return 1
    fi
}

# æ˜¾ç¤ºå¯åŠ¨æ‘˜è¦
show_startup_summary() {
    local start_time=$1
    local end_time=$2
    local duration=$((end_time - start_time))
    
    print_step "å¯åŠ¨æ‘˜è¦"
    
    print_message $GREEN "=========================================="
    print_message $GREEN "       HBaseé›†ç¾¤å¯åŠ¨å®Œæˆï¼"
    print_message $GREEN "=========================================="
    echo
    print_success "å¯åŠ¨è€—æ—¶: ${duration}ç§’"
    print_success "ä¸»å®¹å™¨: $MASTER_CONTAINER"
    print_success "HBaseç›®å½•: $HBASE_DIR"
}

# ä¸»å‡½æ•°
main() {
    local start_time=$(date +%s)
    
    # æ˜¾ç¤ºæ ‡é¢˜
    echo
    print_message $PURPLE "${BOLD}========================================"
    print_message $PURPLE "${BOLD}    HBaseé›†ç¾¤å¯åŠ¨è„šæœ¬ (ä¼˜åŒ–ç‰ˆ)"
    print_message $PURPLE "${BOLD}========================================"
    echo
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print_message $CYAN "é…ç½®ä¿¡æ¯:"
    echo "  - ä¸»å®¹å™¨: $MASTER_CONTAINER"
    echo "  - HBaseç›®å½•: $HBASE_DIR"
    echo "  - å¯åŠ¨è¶…æ—¶: ${STARTUP_TIMEOUT}ç§’"
    echo "  - å¥åº·æ£€æŸ¥é‡è¯•: ${HEALTH_CHECK_RETRIES}æ¬¡"
    echo
    
    # æ‰§è¡Œå¯åŠ¨æ­¥éª¤
    local steps=(
        "check_container:æ£€æŸ¥å®¹å™¨çŠ¶æ€"
        "check_hadoop_cluster:æ£€æŸ¥Hadoopé›†ç¾¤"
        "check_hbase_directory:æ£€æŸ¥HBaseç›®å½•"
        "start_hbase_cluster:å¯åŠ¨HBaseé›†ç¾¤"
        "health_check:å¥åº·æ£€æŸ¥"
    )
    
    local total_steps=${#steps[@]}
    local current_step=0
    local failed_steps=()
    
    for step_info in "${steps[@]}"; do
        current_step=$((current_step + 1))
        local func_name="${step_info%%:*}"
        local description="${step_info##*:}"
        
        print_message $BLUE "[$current_step/$total_steps] $description"
        echo
        
        if ! $func_name; then
            failed_steps+=("$description")
            print_error "æ­¥éª¤å¤±è´¥: $description"
            
            # å¯¹äºå…³é”®æ­¥éª¤å¤±è´¥ï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
            if [[ "$func_name" == "check_container" || "$func_name" == "check_hadoop_cluster" ]]; then
                print_message $YELLOW "å…³é”®æ­¥éª¤å¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ[y/N]"
                read -t 10 -n 1 -r
                echo
                if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                    print_error "ç”¨æˆ·é€‰æ‹©é€€å‡º"
                    exit 1
                fi
            fi
        fi
        echo
    done
    
    local end_time=$(date +%s)
    
    # æ˜¾ç¤ºå¯åŠ¨ç»“æœ
    if [ ${#failed_steps[@]} -eq 0 ]; then
        show_startup_summary $start_time $end_time
    else
        print_message $YELLOW "========================================"
        print_message $YELLOW "    HBaseé›†ç¾¤å¯åŠ¨å®Œæˆï¼ˆæœ‰è­¦å‘Šï¼‰"
        print_message $YELLOW "========================================"
        echo
        print_warning "ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œå¤±è´¥æˆ–æœ‰è­¦å‘Š:"
        for failed_step in "${failed_steps[@]}"; do
            print_error "- $failed_step"
        done
        echo
        print_info "å¯åŠ¨è€—æ—¶: $((end_time - start_time))ç§’"
    fi
    
    # æ˜¾ç¤ºè¿æ¥ä¿¡æ¯
    show_connection_info
    
    # è¯¢é—®å¯é€‰æ“ä½œ
    echo
    print_info "å¯é€‰æ“ä½œ:"
    print_message $YELLOW "1. æ˜¾ç¤ºè¯¦ç»†çŠ¶æ€ [1]"
    print_message $YELLOW "2. æµ‹è¯•åŸºæœ¬åŠŸèƒ½ [2]"
    print_message $YELLOW "3. è·³è¿‡ [ä»»æ„é”®]"
    read -t 10 -n 1 -r
    echo
    
    case $REPLY in
        1)
            show_hbase_status
            ;;
        2)
            test_hbase_functionality
            ;;
        *)
            print_info "è·³è¿‡å¯é€‰æ“ä½œ"
            ;;
    esac
    
    echo
    print_message $CYAN "æç¤º: HBaseé›†ç¾¤ç°åœ¨å¯ä»¥æ¥å—è¿æ¥å’Œè¡¨æ“ä½œ"
    print_message $CYAN "ä½¿ç”¨ 'docker exec -it $MASTER_CONTAINER $HBASE_BIN/hbase shell' è¿æ¥åˆ°HBase Shell"
}

# é”™è¯¯å¤„ç†
set -e
trap 'print_error "è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä½ç½®: $BASH_COMMAND"' ERR

# å…è®¸è„šæœ¬åœ¨æŸäº›å‘½ä»¤å¤±è´¥æ—¶ç»§ç»­è¿è¡Œ
set +e

# è¿è¡Œä¸»å‡½æ•°
main "$@"
```

#### HBase åœæ­¢è„šæœ¬

> [!NOTE]
> æ–‡ä»¶å: stop-hbase.sh

```bash
#!/bin/bash

echo "=== åœæ­¢HBaseé›†ç¾¤ ==="

docker exec hadoop-master1 /opt/hbase/bin/stop-hbase.sh

echo "=== HBaseé›†ç¾¤åœæ­¢å®Œæˆ ==="
```

#### Web ç•Œé¢è®¿é—®

- **HBase Master Web UI**: http://hadoop-master1:16010

## é˜¶æ®µä¸‰ï¼šSpark è®¡ç®—å¼•æ“

Spark æ˜¯ç»Ÿä¸€çš„å¤§æ•°æ®å¤„ç†å¼•æ“ï¼Œæ”¯æŒæ‰¹å¤„ç†ã€æµå¤„ç†ã€SQL æŸ¥è¯¢å’Œæœºå™¨å­¦ä¹ ã€‚

### Spark é›†ç¾¤æ¶æ„

```mermaid
graph TB
    subgraph "Spark Masterå±‚"
        SM1[Spark Master-1<br/>Active]
        SM2[Spark Master-2<br/>Standby]
        SM3[Spark Master-3<br/>Standby]
    end

    subgraph "Spark Workerå±‚"
        SW1[Spark Worker-1<br/>hadoop-worker1]
        SW2[Spark Worker-2<br/>hadoop-worker2]
        SW3[Spark Worker-3<br/>hadoop-worker3]
    end

    subgraph "Sparkåº”ç”¨å±‚"
        APP1[Spark Application 1]
        APP2[Spark Application 2]

        DR1[Driver]
        DR2[Driver]

        EX1[Executor]
        EX2[Executor]
        EX3[Executor]
    end

    subgraph "å­˜å‚¨æ¥å…¥"
        HDFS[HDFS]
        HBASE[HBase]
        LOCAL[Local Files]
    end

    subgraph "åè°ƒæœåŠ¡"
        ZK[ZooKeeper<br/>HAåè°ƒ]
    end

    SM1 --> SW1
    SM1 --> SW2
    SM1 --> SW3

    APP1 --> DR1
    APP2 --> DR2

    DR1 --> EX1
    DR1 --> EX2
    DR2 --> EX3

    SW1 --> EX1
    SW2 --> EX2
    SW3 --> EX3

    EX1 --> HDFS
    EX2 --> HBASE
    EX3 --> LOCAL

    SM1 -.-> ZK
    SM2 -.-> ZK
    SM3 -.-> ZK

    style SM1 fill:#ffcdd2
    style SM2 fill:#e0e0e0
    style SM3 fill:#e0e0e0
    style SW1 fill:#e1f5fe
    style SW2 fill:#e1f5fe
    style SW3 fill:#e1f5fe
    style APP1 fill:#fff3e0
    style APP2 fill:#fff3e0
    style ZK fill:#e8f5e8
```

### æ­¥éª¤ 2ï¼šSpark é…ç½®

#### Spark é…ç½®æ–‡ä»¶

#### spark-defaults.conf

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/spark/conf/spark-defaults.conf

```properties
# åŸºç¡€é…ç½®
spark.master                    spark://hadoop-master1:7077,hadoop-master2:7077,hadoop-master3:7077
spark.eventLog.enabled          true
spark.eventLog.dir              hdfs://mycluster/sparklog/
spark.eventLog.compress         true

# åºåˆ—åŒ–é…ç½®
spark.serializer                org.apache.spark.serializer.KryoSerializer

# SQLé€‚åº”æ€§ä¼˜åŒ–
spark.sql.adaptive.enabled                     true
spark.sql.adaptive.coalescePartitions.enabled  true
spark.sql.adaptive.skewJoin.enabled            true

# åŠ¨æ€èµ„æºåˆ†é…
spark.dynamicAllocation.enabled         true
spark.dynamicAllocation.minExecutors    1
spark.dynamicAllocation.maxExecutors    10
spark.dynamicAllocation.initialExecutors 3

# é«˜å¯ç”¨é…ç½®
spark.deploy.recoveryMode           ZOOKEEPER
spark.deploy.zookeeper.url          zoo1:2181,zoo2:2181,zoo3:2181
spark.deploy.zookeeper.dir          /spark

# å†å²æœåŠ¡å™¨é…ç½®
spark.history.provider              org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory       hdfs://mycluster/sparklog/
spark.history.fs.update.interval    10s

# æ€§èƒ½è°ƒä¼˜
spark.driver.memory                 2g
spark.driver.cores                  2
spark.executor.memory               2g
spark.executor.cores                2
spark.executor.instances            3

# ç½‘ç»œé…ç½®
spark.network.timeout           300s
spark.rpc.askTimeout            300s
```

#### spark-env.sh

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/spark/conf/spark-env.sh

```bash
#!/usr/bin/env bash

# Javaå’ŒHadoopç¯å¢ƒ
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/opt/hadoop
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
export YARN_CONF_DIR=/opt/hadoop/etc/hadoop

# Pythonç¯å¢ƒ
export PYSPARK_PYTHON=/opt/anaconda3/envs/pyspark/bin/python
export PYSPARK_DRIVER_PYTHON=/opt/anaconda3/envs/pyspark/bin/python

# Spark Workeré…ç½®
export SPARK_WORKER_CORES=2
export SPARK_WORKER_MEMORY=2g
export SPARK_WORKER_PORT=7078
export SPARK_WORKER_WEBUI_PORT=8081
export SPARK_WORKER_DIR=/opt/spark/work

# å†å²æœåŠ¡å™¨é…ç½®
export SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://mycluster/sparklog/ \
                           -Dspark.history.fs.cleaner.enabled=true \
                           -Dspark.history.fs.cleaner.interval=1d \
                           -Dspark.history.fs.cleaner.maxAge=7d"

# HAé…ç½®
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER \
                               -Dspark.deploy.zookeeper.url=zoo1:2181,zoo2:2181,zoo3:2181 \
                               -Dspark.deploy.zookeeper.dir=/spark"

# å†…å­˜é…ç½®
export SPARK_DRIVER_MEMORY=2g
export SPARK_EXECUTOR_MEMORY=2g

# æ—¥å¿—é…ç½®
export SPARK_LOG_DIR=/opt/spark/logs
```

#### Workers é…ç½®

> [!NOTE]
> è·¯å¾„: ~/opt/docker-data/hadoop-hbase-spark/spark/conf/workers

```bash
hadoop-worker1
hadoop-worker2
hadoop-worker3
```

### æ­¥éª¤ 3ï¼šå¯åŠ¨ Spark é›†ç¾¤

#### å‡†å¤‡ HDFS ç›®å½•

> [!NOTE]
> æ–‡ä»¶å: spark-init.sh

```bash
#!/bin/bash
# spark-init.sh - Spark åˆå§‹åŒ–è„šæœ¬
# ä¼˜åŒ–ç‰ˆæœ¬ï¼šæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
ORANGE='\033[0;33m'
NC='\033[0m' # No Color

# é…ç½®å˜é‡
CONTAINER_NAME="hadoop-master1"
HDFS_SPARK_DIR="/sparklog"
HADOOP_BIN="/opt/hadoop/bin/hdfs"
SPARK_PERMISSIONS="777"

# æ‰“å°å¸¦é¢œè‰²çš„æ¶ˆæ¯
print_message() {
    local color=$1
    local message=$2
    echo -e "${color}${message}${NC}"
}

# æ‰“å°æˆåŠŸæ¶ˆæ¯
print_success() {
    print_message $GREEN "âœ“ $1"
}

# æ‰“å°è­¦å‘Šæ¶ˆæ¯
print_warning() {
    print_message $YELLOW "âš  $1"
}

# æ‰“å°é”™è¯¯æ¶ˆæ¯
print_error() {
    print_message $RED "âœ— $1"
}

# æ‰“å°ä¿¡æ¯æ¶ˆæ¯
print_info() {
    print_message $BLUE "â„¹ $1"
}

# æ‰“å°å®‰å…¨è­¦å‘Š
print_security_warning() {
    print_message $ORANGE "ğŸ”’ å®‰å…¨è­¦å‘Š: $1"
}

# æ£€æŸ¥Dockerå®¹å™¨æ˜¯å¦è¿è¡Œ
check_container() {
    print_info "æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€..."
    
    if ! command -v docker &> /dev/null; then
        print_error "Docker æœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­"
        return 1
    fi
    
    if ! docker ps | grep -q "$CONTAINER_NAME"; then
        print_error "å®¹å™¨ '$CONTAINER_NAME' æœªè¿è¡Œ"
        print_info "è¯·å…ˆå¯åŠ¨Hadoopé›†ç¾¤å®¹å™¨"
        return 1
    fi
    
    print_success "å®¹å™¨ '$CONTAINER_NAME' æ­£åœ¨è¿è¡Œ"
    return 0
}

# æ£€æŸ¥HDFSæœåŠ¡çŠ¶æ€
check_hdfs_status() {
    print_info "æ£€æŸ¥HDFSæœåŠ¡çŠ¶æ€..."
    
    if ! docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -ls / &>/dev/null; then
        print_error "HDFSæœåŠ¡æœªæ­£å¸¸è¿è¡Œ"
        print_info "è¯·ç¡®ä¿Hadoopé›†ç¾¤å·²æ­£ç¡®å¯åŠ¨"
        return 1
    fi
    
    print_success "HDFSæœåŠ¡æ­£å¸¸"
    return 0
}

# æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
check_directory_exists() {
    local dir_path=$1
    docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -test -d "$dir_path" 2>/dev/null
}

# æ˜¾ç¤ºæƒé™å®‰å…¨è­¦å‘Š
show_security_warning() {
    echo
    print_security_warning "æƒé™è®¾ç½®è­¦å‘Š"
    echo
    print_message $ORANGE "å³å°†è®¾ç½®ç›®å½•æƒé™ä¸º 777 (rwxrwxrwx)"
    print_message $ORANGE "è¿™æ„å‘³ç€ï¼š"
    echo "  - æ‰€æœ‰ç”¨æˆ·éƒ½å¯ä»¥è¯»å–ã€å†™å…¥å’Œæ‰§è¡Œ"
    echo "  - è¿™å¯èƒ½å­˜åœ¨å®‰å…¨é£é™©"
    echo "  - å»ºè®®ä»…åœ¨å¼€å‘ç¯å¢ƒä¸­ä½¿ç”¨"
    echo
    print_message $YELLOW "æ˜¯å¦ç»§ç»­è®¾ç½® 777 æƒé™ï¼Ÿ[y/N]"
    read -t 15 -n 1 -r
    echo
    
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        print_info "ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œå°†è®¾ç½®æ›´å®‰å…¨çš„ 755 æƒé™"
        SPARK_PERMISSIONS="755"
        return 1
    fi
    
    print_warning "ç”¨æˆ·ç¡®è®¤ä½¿ç”¨ 777 æƒé™"
    return 0
}

# åˆ›å»ºHDFSç›®å½•
create_hdfs_directory() {
    print_info "åˆ›å»ºHDFSç›®å½•: $HDFS_SPARK_DIR"
    
    if check_directory_exists "$HDFS_SPARK_DIR"; then
        print_warning "ç›®å½• '$HDFS_SPARK_DIR' å·²å­˜åœ¨"
        return 0
    fi
    
    if docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -mkdir -p "$HDFS_SPARK_DIR"; then
        print_success "ç›®å½•åˆ›å»ºæˆåŠŸ: $HDFS_SPARK_DIR"
        return 0
    else
        print_error "ç›®å½•åˆ›å»ºå¤±è´¥: $HDFS_SPARK_DIR"
        return 1
    fi
}

# è®¾ç½®ç›®å½•æƒé™
set_directory_permissions() {
    print_info "è®¾ç½®ç›®å½•æƒé™: $HDFS_SPARK_DIR ($SPARK_PERMISSIONS)"
    
    if docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -chmod "$SPARK_PERMISSIONS" "$HDFS_SPARK_DIR"; then
        print_success "æƒé™è®¾ç½®æˆåŠŸ: $SPARK_PERMISSIONS"
        return 0
    else
        print_error "æƒé™è®¾ç½®å¤±è´¥"
        return 1
    fi
}

# éªŒè¯ç›®å½•æƒé™
verify_permissions() {
    print_info "éªŒè¯ç›®å½•æƒé™..."
    
    local permissions=$(docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -ls / 2>/dev/null | grep "sparklog" | awk '{print $1}')
    
    if [[ -n "$permissions" ]]; then
        print_success "æƒé™éªŒè¯é€šè¿‡: $permissions"
        
        # æ£€æŸ¥æ˜¯å¦ä¸º777æƒé™å¹¶å†æ¬¡æé†’å®‰å…¨é£é™©
        if [[ $permissions =~ rwxrwxrwx ]]; then
            print_security_warning "å½“å‰ä½¿ç”¨777æƒé™ï¼Œè¯·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è€ƒè™‘æ›´å®‰å…¨çš„æƒé™è®¾ç½®"
        fi
        return 0
    else
        print_warning "æ— æ³•è·å–æƒé™ä¿¡æ¯"
        return 1
    fi
}

# æ˜¾ç¤ºç›®å½•ä¿¡æ¯
show_directory_info() {
    print_info "æ˜¾ç¤ºSparkæ—¥å¿—ç›®å½•ä¿¡æ¯:"
    echo
    print_message $CYAN "ç›®å½•è¯¦æƒ…:"
    docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -ls -d "$HDFS_SPARK_DIR" 2>/dev/null || {
        print_error "æ— æ³•è·å–ç›®å½•ä¿¡æ¯"
        return 1
    }
    echo
}

# æµ‹è¯•ç›®å½•è®¿é—®æƒé™
test_directory_access() {
    print_info "æµ‹è¯•ç›®å½•è®¿é—®æƒé™..."
    
    # å°è¯•åœ¨ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªæµ‹è¯•æ–‡ä»¶
    local test_file="$HDFS_SPARK_DIR/spark-init-test-$(date +%s)"
    
    if docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -touchz "$test_file" 2>/dev/null; then
        print_success "ç›®å½•å†™å…¥æµ‹è¯•æˆåŠŸ"
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -rm "$test_file" 2>/dev/null
        return 0
    else
        print_error "ç›®å½•å†™å…¥æµ‹è¯•å¤±è´¥"
        return 1
    fi
}

# æ˜¾ç¤ºSparké…ç½®å»ºè®®
show_spark_config_tips() {
    print_info "Sparké…ç½®å»ºè®®:"
    echo
    print_message $CYAN "åœ¨Sparké…ç½®ä¸­è®¾ç½®ä»¥ä¸‹å‚æ•°ï¼š"
    echo "  spark.eventLog.enabled=true"
    echo "  spark.eventLog.dir=hdfs://namenode:9000$HDFS_SPARK_DIR"
    echo "  spark.history.fs.logDirectory=hdfs://namenode:9000$HDFS_SPARK_DIR"
    echo
    print_message $CYAN "è¿™å°†å¯ç”¨Sparkäº‹ä»¶æ—¥å¿—è®°å½•åŠŸèƒ½"
}

# æ˜¾ç¤ºHDFSæ¦‚è§ˆ
show_hdfs_overview() {
    print_info "HDFSæ–‡ä»¶ç³»ç»Ÿæ¦‚è§ˆ:"
    echo
    print_message $CYAN "æ ¹ç›®å½•å†…å®¹:"
    docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -ls / 2>/dev/null || {
        print_error "æ— æ³•è·å–HDFSä¿¡æ¯"
        return 1
    }
    echo
}

# ä¸»å‡½æ•°
main() {
    # æ˜¾ç¤ºæ ‡é¢˜
    echo
    print_message $PURPLE "========================================"
    print_message $PURPLE "    Spark HDFS åˆå§‹åŒ–è„šæœ¬ (ä¼˜åŒ–ç‰ˆ)"
    print_message $PURPLE "========================================"
    echo
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print_message $CYAN "é…ç½®ä¿¡æ¯:"
    echo "  - å®¹å™¨åç§°: $CONTAINER_NAME"
    echo "  - Sparkæ—¥å¿—ç›®å½•: $HDFS_SPARK_DIR"
    echo "  - HadoopäºŒè¿›åˆ¶: $HADOOP_BIN"
    echo "  - é»˜è®¤æƒé™: $SPARK_PERMISSIONS"
    echo
    
    # æ˜¾ç¤ºæƒé™å®‰å…¨è­¦å‘Š
    if [[ "$SPARK_PERMISSIONS" == "777" ]]; then
        show_security_warning
    fi
    
    # æ‰§è¡Œæ£€æŸ¥å’Œåˆå§‹åŒ–æ­¥éª¤
    local steps=(
        "check_container:æ£€æŸ¥Dockerå®¹å™¨"
        "check_hdfs_status:æ£€æŸ¥HDFSçŠ¶æ€"
        "create_hdfs_directory:åˆ›å»ºSparkæ—¥å¿—ç›®å½•"
        "set_directory_permissions:è®¾ç½®ç›®å½•æƒé™"
        "verify_permissions:éªŒè¯æƒé™è®¾ç½®"
        "test_directory_access:æµ‹è¯•ç›®å½•è®¿é—®"
        "show_directory_info:æ˜¾ç¤ºç›®å½•ä¿¡æ¯"
    )
    
    local total_steps=${#steps[@]}
    local current_step=0
    local failed_steps=()
    
    for step_info in "${steps[@]}"; do
        current_step=$((current_step + 1))
        local func_name="${step_info%%:*}"
        local description="${step_info##*:}"
        
        print_message $BLUE "[$current_step/$total_steps] $description"
        
        if ! $func_name; then
            failed_steps+=("$description")
            print_error "æ­¥éª¤å¤±è´¥: $description"
            
            # å¯¹äºå…³é”®æ­¥éª¤å¤±è´¥ï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
            if [[ "$func_name" == "check_container" || "$func_name" == "check_hdfs_status" ]]; then
                print_message $YELLOW "å…³é”®æ­¥éª¤å¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ[y/N]"
                read -t 10 -n 1 -r
                echo
                if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                    print_error "ç”¨æˆ·é€‰æ‹©é€€å‡º"
                    exit 1
                fi
            fi
        fi
        echo
    done
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    if [ ${#failed_steps[@]} -eq 0 ]; then
        print_message $GREEN "========================================"
        print_message $GREEN "       Spark åˆå§‹åŒ–æˆåŠŸå®Œæˆï¼"
        print_message $GREEN "========================================"
        echo
        print_success "æ‰€æœ‰æ­¥éª¤å‡å·²æˆåŠŸæ‰§è¡Œ"
    else
        print_message $YELLOW "========================================"
        print_message $YELLOW "    Spark åˆå§‹åŒ–å®Œæˆï¼ˆæœ‰è­¦å‘Šï¼‰"
        print_message $YELLOW "========================================"
        echo
        print_warning "ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œå¤±è´¥æˆ–æœ‰è­¦å‘Š:"
        for failed_step in "${failed_steps[@]}"; do
            print_error "- $failed_step"
        done
    fi
    
    # æ˜¾ç¤ºé…ç½®å»ºè®®
    echo
    show_spark_config_tips
    
    echo
    print_info "å¯é€‰æ“ä½œ:"
    print_message $YELLOW "1. æ˜¾ç¤ºHDFSæ¦‚è§ˆ [1]"
    print_message $YELLOW "2. æ˜¾ç¤ºç›®å½•å¤§å° [2]"
    print_message $YELLOW "3. è·³è¿‡ [ä»»æ„é”®]"
    read -t 8 -n 1 -r
    echo
    
    case $REPLY in
        1)
            show_hdfs_overview
            ;;
        2)
            print_info "ç›®å½•å¤§å°ä¿¡æ¯:"
            docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -du -h "$HDFS_SPARK_DIR" 2>/dev/null || print_warning "ç›®å½•ä¸ºç©ºæˆ–æ— æ³•è®¿é—®"
            ;;
        *)
            print_info "è·³è¿‡å¯é€‰æ“ä½œ"
            ;;
    esac
    
    echo
    print_message $CYAN "æç¤º: Sparkç°åœ¨å¯ä»¥ä½¿ç”¨HDFSç›®å½• '$HDFS_SPARK_DIR' å­˜å‚¨äº‹ä»¶æ—¥å¿—"
    print_message $CYAN "ä¸‹ä¸€æ­¥: é…ç½®Sparkä»¥å¯ç”¨äº‹ä»¶æ—¥å¿—è®°å½•"
}

# é”™è¯¯å¤„ç†
set -e
trap 'print_error "è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä½ç½®: $BASH_COMMAND"' ERR

# å…è®¸è„šæœ¬åœ¨æŸäº›å‘½ä»¤å¤±è´¥æ—¶ç»§ç»­è¿è¡Œ
set +e

# è¿è¡Œä¸»å‡½æ•°
main "$@"
```

#### Spark å¯åŠ¨è„šæœ¬

#### SparkStandaloneHAå¯åŠ¨è„šæœ¬

> [!NOTE]
> æ–‡ä»¶å: start-spark.sh

```bash
#!/bin/bash
# start-spark.sh - Sparké›†ç¾¤å¯åŠ¨è„šæœ¬ (HAæ¨¡å¼)
# ä¼˜åŒ–ç‰ˆæœ¬ï¼šæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
BOLD='\033[1m'
NC='\033[0m' # No Color

# é›†ç¾¤é…ç½®
MASTER_NODES=("hadoop-master1" "hadoop-master2" "hadoop-master3")
WORKER_NODES=("hadoop-worker1" "hadoop-worker2" "hadoop-worker3")
ALL_NODES=("${MASTER_NODES[@]}" "${WORKER_NODES[@]}")

# æœåŠ¡é…ç½®
SPARK_BIN="/opt/spark/bin"
SPARK_SBIN="/opt/spark/sbin"
SPARK_LOG_DIR="/sparklog"
STARTUP_TIMEOUT=120
HEALTH_CHECK_RETRIES=5

# æ‰“å°å¸¦é¢œè‰²çš„æ¶ˆæ¯
print_message() {
    local color=$1
    local message=$2
    echo -e "${color}${message}${NC}"
}

# æ‰“å°æˆåŠŸæ¶ˆæ¯
print_success() {
    print_message $GREEN "âœ“ $1"
}

# æ‰“å°è­¦å‘Šæ¶ˆæ¯
print_warning() {
    print_message $YELLOW "âš  $1"
}

# æ‰“å°é”™è¯¯æ¶ˆæ¯
print_error() {
    print_message $RED "âœ— $1"
}

# æ‰“å°ä¿¡æ¯æ¶ˆæ¯
print_info() {
    print_message $BLUE "â„¹ $1"
}

# æ‰“å°æ­¥éª¤æ ‡é¢˜
print_step() {
    print_message $PURPLE "${BOLD}=== $1 ===${NC}"
}

# æ˜¾ç¤ºå¯åŠ¨åŠ¨ç”»
show_progress() {
    local duration=$1
    local description=$2
    local chars="â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â "
    local delay=0.1
    local i=0
    
    while [ $i -lt $duration ]; do
        for (( j=0; j<${#chars}; j++ )); do
            printf "\r${BLUE}${chars:$j:1} $description... ${CYAN}(%ds)${NC}" $((duration - i))
            sleep $delay
            i=$((i + 1))
            if [ $i -ge $duration ]; then
                break
            fi
        done
    done
    printf "\r${GREEN}âœ“ $description å®Œæˆ${NC}\n"
}

# æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€
check_containers() {
    print_step "æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€"
    
    if ! command -v docker &> /dev/null; then
        print_error "Docker æœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­"
        return 1
    fi
    
    local failed_containers=()
    
    for node in "${ALL_NODES[@]}"; do
        if docker ps | grep -q "$node"; then
            print_success "$node å®¹å™¨æ­£åœ¨è¿è¡Œ"
        else
            print_error "$node å®¹å™¨æœªè¿è¡Œ"
            failed_containers+=("$node")
        fi
    done
    
    if [ ${#failed_containers[@]} -gt 0 ]; then
        print_error "ä»¥ä¸‹å®¹å™¨æœªè¿è¡Œ: ${failed_containers[*]}"
        print_info "è¯·å…ˆå¯åŠ¨æ‰€æœ‰å®¹å™¨"
        return 1
    fi
    
    print_success "æ‰€æœ‰å®¹å™¨çŠ¶æ€æ­£å¸¸"
    return 0
}

# æ£€æŸ¥HDFSå’ŒSparkæ—¥å¿—ç›®å½•
check_spark_dependencies() {
    print_step "æ£€æŸ¥Sparkä¾èµ–"
    
    # æ£€æŸ¥HDFSæ˜¯å¦å¯è®¿é—®
    print_info "æ£€æŸ¥HDFSæœåŠ¡..."
    if ! docker exec "${MASTER_NODES[0]}" /opt/hadoop/bin/hdfs dfs -ls / &>/dev/null; then
        print_warning "HDFSæœåŠ¡ä¸å¯ç”¨ï¼ŒSparkå†å²æœåŠ¡å™¨å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ"
    else
        print_success "HDFSæœåŠ¡æ­£å¸¸"
    fi
    
    # æ£€æŸ¥Sparkæ—¥å¿—ç›®å½•
    print_info "æ£€æŸ¥Sparkæ—¥å¿—ç›®å½•..."
    if docker exec "${MASTER_NODES[0]}" /opt/hadoop/bin/hdfs dfs -test -d "$SPARK_LOG_DIR" 2>/dev/null; then
        print_success "Sparkæ—¥å¿—ç›®å½• '$SPARK_LOG_DIR' å­˜åœ¨"
    else
        print_warning "Sparkæ—¥å¿—ç›®å½• '$SPARK_LOG_DIR' ä¸å­˜åœ¨"
        print_info "å»ºè®®å…ˆè¿è¡Œ spark-init.sh åˆå§‹åŒ–Sparkç›®å½•"
        
        print_message $YELLOW "æ˜¯å¦è‡ªåŠ¨åˆ›å»ºSparkæ—¥å¿—ç›®å½•ï¼Ÿ[y/N]"
        read -t 10 -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            if docker exec "${MASTER_NODES[0]}" /opt/hadoop/bin/hdfs dfs -mkdir -p "$SPARK_LOG_DIR" && \
               docker exec "${MASTER_NODES[0]}" /opt/hadoop/bin/hdfs dfs -chmod 777 "$SPARK_LOG_DIR"; then
                print_success "Sparkæ—¥å¿—ç›®å½•åˆ›å»ºæˆåŠŸ"
            else
                print_error "Sparkæ—¥å¿—ç›®å½•åˆ›å»ºå¤±è´¥"
            fi
        fi
    fi
    
    return 0
}

# æ‰§è¡Œå‘½ä»¤å¹¶æ£€æŸ¥ç»“æœ
execute_command() {
    local node=$1
    local command=$2
    local description=$3
    
    if docker exec "$node" $command &>/dev/null; then
        print_success "$node: $description"
        return 0
    else
        print_error "$node: $description å¤±è´¥"
        return 1
    fi
}

# å¯åŠ¨Spark MasterèŠ‚ç‚¹
start_spark_masters() {
    print_step "å¯åŠ¨Spark MasterèŠ‚ç‚¹ (HAæ¨¡å¼)"
    
    local failed_masters=()
    
    for master in "${MASTER_NODES[@]}"; do
        print_info "å¯åŠ¨ $master ä¸Šçš„Spark Master..."
        if execute_command "$master" "$SPARK_SBIN/start-master.sh" "å¯åŠ¨Spark Master"; then
            continue
        else
            failed_masters+=("$master")
        fi
    done
    
    if [ ${#failed_masters[@]} -gt 0 ]; then
        print_warning "ä»¥ä¸‹MasterèŠ‚ç‚¹å¯åŠ¨å¤±è´¥: ${failed_masters[*]}"
    fi
    
    # ç­‰å¾…Masterå¯åŠ¨
    show_progress 10 "ç­‰å¾…Spark MasterèŠ‚ç‚¹å¯åŠ¨"
    
    return 0
}

# å¯åŠ¨Spark WorkerèŠ‚ç‚¹
start_spark_workers() {
    print_step "å¯åŠ¨Spark WorkerèŠ‚ç‚¹"
    
    print_info "ä» ${MASTER_NODES[0]} å¯åŠ¨æ‰€æœ‰WorkerèŠ‚ç‚¹..."
    
    if docker exec "${MASTER_NODES[0]}" "$SPARK_SBIN/start-workers.sh"; then
        print_success "Spark Workerå¯åŠ¨å‘½ä»¤æ‰§è¡ŒæˆåŠŸ"
    else
        print_error "Spark Workerå¯åŠ¨å¤±è´¥"
        return 1
    fi
    
    # ç­‰å¾…Workerå¯åŠ¨
    show_progress 8 "ç­‰å¾…Spark WorkerèŠ‚ç‚¹å¯åŠ¨"
    
    return 0
}

# å¯åŠ¨Sparkå†å²æœåŠ¡å™¨
start_spark_history_server() {
    print_step "å¯åŠ¨Sparkå†å²æœåŠ¡å™¨"
    
    print_info "å¯åŠ¨Sparkå†å²æœåŠ¡å™¨..."
    
    if docker exec "${MASTER_NODES[0]}" "$SPARK_SBIN/start-history-server.sh"; then
        print_success "Sparkå†å²æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ"
    else
        print_warning "Sparkå†å²æœåŠ¡å™¨å¯åŠ¨å¤±è´¥"
        return 1
    fi
    
    # ç­‰å¾…å†å²æœåŠ¡å™¨å¯åŠ¨
    show_progress 5 "ç­‰å¾…Sparkå†å²æœåŠ¡å™¨å¯åŠ¨"
    
    return 0
}

# å¥åº·æ£€æŸ¥
health_check() {
    print_step "Sparké›†ç¾¤å¥åº·æ£€æŸ¥"
    
    local checks=(
        "check_spark_masters:æ£€æŸ¥Spark MasterçŠ¶æ€"
        "check_spark_workers:æ£€æŸ¥Spark WorkerçŠ¶æ€"
        "check_spark_version:æ£€æŸ¥Sparkç‰ˆæœ¬"
        "check_spark_ui:æ£€æŸ¥Spark Web UI"
    )
    
    for check_info in "${checks[@]}"; do
        local func_name="${check_info%%:*}"
        local description="${check_info##*:}"
        
        print_info "$description..."
        if $func_name; then
            print_success "$description é€šè¿‡"
        else
            print_warning "$description å¤±è´¥æˆ–å¼‚å¸¸"
        fi
    done
}

# æ£€æŸ¥Spark MasterçŠ¶æ€
check_spark_masters() {
    local active_masters=0
    
    for master in "${MASTER_NODES[@]}"; do
        if docker exec "$master" "$SPARK_BIN/spark-submit" --version &>/dev/null; then
            active_masters=$((active_masters + 1))
        fi
    done
    
    print_info "å‘ç° $active_masters ä¸ªæ´»è·ƒçš„Spark Master"
    [ $active_masters -gt 0 ]
}

# æ£€æŸ¥Spark WorkerçŠ¶æ€
check_spark_workers() {
    # å°è¯•ä»Spark Masterè·å–Workerä¿¡æ¯
    local worker_info
    if worker_info=$(docker exec "${MASTER_NODES[0]}" curl -s http://hadoop-master1:8080 2>/dev/null); then
        if echo "$worker_info" | grep -q "worker"; then
            return 0
        fi
    fi
    return 1
}

# æ£€æŸ¥Sparkç‰ˆæœ¬
check_spark_version() {
    docker exec "${MASTER_NODES[0]}" "$SPARK_BIN/spark-submit" --version &>/dev/null
}

# æ£€æŸ¥Spark Web UI
check_spark_ui() {
    docker exec "${MASTER_NODES[0]}" curl -s http://hadoop-master1:8080 &>/dev/null
}

# æ˜¾ç¤ºé›†ç¾¤çŠ¶æ€
show_cluster_status() {
    print_step "Sparké›†ç¾¤çŠ¶æ€è¯¦æƒ…"
    
    echo
    print_message $CYAN "Sparkç‰ˆæœ¬ä¿¡æ¯:"
    docker exec "${MASTER_NODES[0]}" "$SPARK_BIN/spark-submit" --version 2>/dev/null || print_warning "æ— æ³•è·å–Sparkç‰ˆæœ¬"
    
    echo
    print_message $CYAN "Spark MasterèŠ‚ç‚¹çŠ¶æ€:"
    for master in "${MASTER_NODES[@]}"; do
        if docker exec "$master" pgrep -f "org.apache.spark.deploy.master.Master" &>/dev/null; then
            print_success "$master: Masterè¿›ç¨‹è¿è¡Œä¸­"
        else
            print_warning "$master: Masterè¿›ç¨‹æœªè¿è¡Œ"
        fi
    done
    
    echo
    print_message $CYAN "Spark WorkerèŠ‚ç‚¹çŠ¶æ€:"
    for worker in "${WORKER_NODES[@]}"; do
        if docker exec "$worker" pgrep -f "org.apache.spark.deploy.worker.Worker" &>/dev/null; then
            print_success "$worker: Workerè¿›ç¨‹è¿è¡Œä¸­"
        else
            print_warning "$worker: Workerè¿›ç¨‹æœªè¿è¡Œ"
        fi
    done
}

# æ˜¾ç¤ºè¿æ¥ä¿¡æ¯
show_connection_info() {
    print_step "è¿æ¥ä¿¡æ¯"
    
    echo
    print_message $CYAN "Spark Webè®¿é—®åœ°å€:"
    echo "  - Master UI (ä¸»): http://hadoop-master1:8080"
    echo "  - Master UI (å¤‡1): http://hadoop-master2:8080"  
    echo "  - Master UI (å¤‡2): http://hadoop-master3:8080"
    echo "  - History Server: http://hadoop-master1:18080"
    
    echo
    print_message $CYAN "Spark Shellè¿æ¥å‘½ä»¤:"
    echo "  docker exec -it ${MASTER_NODES[0]} $SPARK_BIN/spark-shell"
    echo "  docker exec -it ${MASTER_NODES[0]} $SPARK_BIN/pyspark"
    
    echo
    print_message $CYAN "Spark Submitç¤ºä¾‹:"
    echo "  docker exec ${MASTER_NODES[0]} $SPARK_BIN/spark-submit \\"
    echo "    --class org.apache.spark.examples.SparkPi \\"
    echo "    --master spark://hadoop-master1:7077 \\"
    echo "    /opt/spark/examples/jars/spark-examples_*.jar 10"
}

# æµ‹è¯•SparkåŸºæœ¬åŠŸèƒ½
test_spark_functionality() {
    print_info "æµ‹è¯•SparkåŸºæœ¬åŠŸèƒ½..."
    
    # è¿è¡ŒSpark Piç¤ºä¾‹
    local test_command="$SPARK_BIN/spark-submit --class org.apache.spark.examples.SparkPi --master spark://hadoop-master1:7077 /opt/spark/examples/jars/spark-examples_*.jar 2"
    
    if docker exec "${MASTER_NODES[0]}" timeout 60 $test_command &>/dev/null; then
        print_success "SparkåŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡ (SparkPi)"
        return 0
    else
        print_warning "SparkåŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥"
        return 1
    fi
}

# æ˜¾ç¤ºå¯åŠ¨æ‘˜è¦
show_startup_summary() {
    local start_time=$1
    local end_time=$2
    local duration=$((end_time - start_time))
    
    print_step "å¯åŠ¨æ‘˜è¦"
    
    print_message $GREEN "=========================================="
    print_message $GREEN "       Sparké›†ç¾¤å¯åŠ¨å®Œæˆï¼"
    print_message $GREEN "=========================================="
    echo
    print_success "å¯åŠ¨è€—æ—¶: ${duration}ç§’"
    print_success "é›†ç¾¤æ¨¡å¼: HA (é«˜å¯ç”¨)"
    print_success "MasterèŠ‚ç‚¹: ${#MASTER_NODES[@]} ä¸ª"
    print_success "WorkerèŠ‚ç‚¹: ${#WORKER_NODES[@]} ä¸ª"
}

# ä¸»å‡½æ•°
main() {
    local start_time=$(date +%s)
    
    # æ˜¾ç¤ºæ ‡é¢˜
    echo
    print_message $PURPLE "${BOLD}========================================"
    print_message $PURPLE "${BOLD}    Sparké›†ç¾¤å¯åŠ¨è„šæœ¬ (HAæ¨¡å¼ä¼˜åŒ–ç‰ˆ)"
    print_message $PURPLE "${BOLD}========================================"
    echo
    
    # æ˜¾ç¤ºé›†ç¾¤é…ç½®
    print_message $CYAN "é›†ç¾¤é…ç½®ä¿¡æ¯:"
    echo "  - MasterèŠ‚ç‚¹: ${MASTER_NODES[*]}"
    echo "  - WorkerèŠ‚ç‚¹: ${WORKER_NODES[*]}"
    echo "  - è¿è¡Œæ¨¡å¼: é«˜å¯ç”¨ (HA)"
    echo "  - å¯åŠ¨è¶…æ—¶: ${STARTUP_TIMEOUT}ç§’"
    echo "  - æ—¥å¿—ç›®å½•: $SPARK_LOG_DIR"
    echo
    
    # æ‰§è¡Œå¯åŠ¨æ­¥éª¤
    local steps=(
        "check_containers:æ£€æŸ¥å®¹å™¨çŠ¶æ€"
        "check_spark_dependencies:æ£€æŸ¥Sparkä¾èµ–"
        "start_spark_masters:å¯åŠ¨Spark Masters"
        "start_spark_workers:å¯åŠ¨Spark Workers"
        "start_spark_history_server:å¯åŠ¨å†å²æœåŠ¡å™¨"
        "health_check:å¥åº·æ£€æŸ¥"
    )
    
    local total_steps=${#steps[@]}
    local current_step=0
    local failed_steps=()
    
    for step_info in "${steps[@]}"; do
        current_step=$((current_step + 1))
        local func_name="${step_info%%:*}"
        local description="${step_info##*:}"
        
        print_message $BLUE "[$current_step/$total_steps] $description"
        echo
        
        if ! $func_name; then
            failed_steps+=("$description")
            print_error "æ­¥éª¤å¤±è´¥: $description"
            
            # å¯¹äºå…³é”®æ­¥éª¤å¤±è´¥ï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
            if [[ "$func_name" == "check_containers" ]]; then
                print_message $YELLOW "å…³é”®æ­¥éª¤å¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ[y/N]"
                read -t 10 -n 1 -r
                echo
                if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                    print_error "ç”¨æˆ·é€‰æ‹©é€€å‡º"
                    exit 1
                fi
            fi
        fi
        echo
    done
    
    local end_time=$(date +%s)
    
    # æ˜¾ç¤ºå¯åŠ¨ç»“æœ
    if [ ${#failed_steps[@]} -eq 0 ]; then
        show_startup_summary $start_time $end_time
    else
        print_message $YELLOW "========================================"
        print_message $YELLOW "    Sparké›†ç¾¤å¯åŠ¨å®Œæˆï¼ˆæœ‰è­¦å‘Šï¼‰"
        print_message $YELLOW "========================================"
        echo
        print_warning "ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œå¤±è´¥æˆ–æœ‰è­¦å‘Š:"
        for failed_step in "${failed_steps[@]}"; do
            print_error "- $failed_step"
        done
        echo
        print_info "å¯åŠ¨è€—æ—¶: $((end_time - start_time))ç§’"
    fi
    
    # æ˜¾ç¤ºè¿æ¥ä¿¡æ¯
    show_connection_info
    
    # è¯¢é—®å¯é€‰æ“ä½œ
    echo
    print_info "å¯é€‰æ“ä½œ:"
    print_message $YELLOW "1. æ˜¾ç¤ºè¯¦ç»†çŠ¶æ€ [1]"
    print_message $YELLOW "2. æµ‹è¯•åŸºæœ¬åŠŸèƒ½ [2]"
    print_message $YELLOW "3. è·³è¿‡ [ä»»æ„é”®]"
    read -t 10 -n 1 -r
    echo
    
    case $REPLY in
        1)
            show_cluster_status
            ;;
        2)
            test_spark_functionality
            ;;
        *)
            print_info "è·³è¿‡å¯é€‰æ“ä½œ"
            ;;
    esac
    
    echo
    print_message $CYAN "æç¤º: Sparké›†ç¾¤ç°åœ¨å¯ä»¥æ¥å—ä½œä¸šæäº¤"
    print_message $CYAN "è®¿é—® http://hadoop-master1:8080 æŸ¥çœ‹Master Web UI"
}

# é”™è¯¯å¤„ç†
set -e
trap 'print_error "è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä½ç½®: $BASH_COMMAND"' ERR

# å…è®¸è„šæœ¬åœ¨æŸäº›å‘½ä»¤å¤±è´¥æ—¶ç»§ç»­è¿è¡Œ
set +e

# è¿è¡Œä¸»å‡½æ•°
main "$@"
```

#### Sparkåœæ­¢è„šæœ¬

> [!NOTE]
> æ–‡ä»¶å: stop-spark.sh

```bash
#!/bin/bash

echo "=== åœæ­¢Sparké›†ç¾¤ ==="

# åœæ­¢å†å²æœåŠ¡å™¨
echo "åœæ­¢å†å²æœåŠ¡å™¨..."
docker exec hadoop-master1 /opt/spark/sbin/stop-history-server.sh

# åœæ­¢æ‰€æœ‰Worker
echo "åœæ­¢WorkerèŠ‚ç‚¹..."
docker exec hadoop-master1 /opt/spark/sbin/stop-workers.sh

# åœæ­¢æ‰€æœ‰Master
echo "åœæ­¢MasterèŠ‚ç‚¹..."
docker exec hadoop-master3 /opt/spark/sbin/stop-master.sh
docker exec hadoop-master2 /opt/spark/sbin/stop-master.sh
docker exec hadoop-master1 /opt/spark/sbin/stop-master.sh

echo "=== Sparké›†ç¾¤åœæ­¢å®Œæˆ ==="
```

### æ­¥éª¤ 4ï¼šSpark æ¨¡å¼é€‰æ‹©

#### Local æ¨¡å¼ï¼ˆå¼€å‘æµ‹è¯•ï¼‰

```bash
# è¿›å…¥Spark Localæ¨¡å¼
docker exec -it hadoop-master1 /opt/spark/bin/pyspark

# Pythonä»£ç æµ‹è¯•
sc.parallelize([1,2,3,4,5]).map(lambda x: x*10).collect()
```

#### Standalone æ¨¡å¼ï¼ˆé›†ç¾¤éƒ¨ç½²ï¼‰

```bash
# æäº¤Sparkåº”ç”¨åˆ°é›†ç¾¤
docker exec hadoop-master1 /opt/spark/bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://hadoop-master1:7077,hadoop-master2:7077,hadoop-master3:7077 \
  --deploy-mode client \
  --driver-memory 1g \
  --executor-memory 1g \
  --executor-cores 1 \
  /opt/spark/examples/jars/spark-examples_2.12-3.4.1.jar \
  10
```

#### PySpark é›†ç¾¤æ¨¡å¼

```bash
# å¯åŠ¨PySparkè¿æ¥åˆ°é›†ç¾¤
docker exec -it hadoop-master1 /opt/spark/bin/pyspark \
  --master spark://hadoop-master1:7077,hadoop-master2:7077,hadoop-master3:7077 \
  --driver-memory 1g \
  --executor-memory 1g \
  --executor-cores 1
```

### æ­¥éª¤ 5ï¼šéªŒè¯ Spark

#### é›†æˆæµ‹è¯•è„šæœ¬

> [!NOTE]
> æ–‡ä»¶å: spark-integration-test.py

```python
from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf
import time

def test_spark_cluster():
    print("=== Sparké›†ç¾¤é›†æˆæµ‹è¯• ===")

    # åˆ›å»ºSparkä¼šè¯
    spark = SparkSession.builder \
        .appName("BigDataPlatformTest") \
        .config("spark.master", "spark://hadoop-master1:7077,hadoop-master2:7077,hadoop-master3:7077") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.memory", "1g") \
        .getOrCreate()

    sc = spark.sparkContext

    # æµ‹è¯•1: åŸºæœ¬RDDæ“ä½œ
    print("æµ‹è¯•1: åŸºæœ¬RDDæ“ä½œ")
    data = range(1, 1000000)
    rdd = sc.parallelize(data, 10)
    result = rdd.map(lambda x: x * 2).filter(lambda x: x % 1000 == 0).count()
    print(f"è¿‡æ»¤ç»“æœæ•°é‡: {result}")

    # æµ‹è¯•2: HDFSè¯»å†™
    print("æµ‹è¯•2: HDFSè¯»å†™æµ‹è¯•")
    try:
        # å†™å…¥HDFS
        test_rdd = sc.parallelize(["Hello", "World", "Spark", "Hadoop", "HBase"])
        test_rdd.saveAsTextFile("hdfs://mycluster/tmp/spark-test")

        # ä»HDFSè¯»å–
        read_rdd = sc.textFile("hdfs://mycluster/tmp/spark-test")
        read_data = read_rdd.collect()
        print(f"ä»HDFSè¯»å–çš„æ•°æ®: {read_data}")

    except Exception as e:
        print(f"HDFSæµ‹è¯•å¤±è´¥: {e}")

    # æµ‹è¯•3: Spark SQL
    print("æµ‹è¯•3: Spark SQLæµ‹è¯•")
    df = spark.createDataFrame([
        (1, "Alice", 25),
        (2, "Bob", 30),
        (3, "Charlie", 35)
    ], ["id", "name", "age"])

    df.createOrReplaceTempView("people")
    result_df = spark.sql("SELECT name, age FROM people WHERE age > 28")
    result_df.show()

    # æµ‹è¯•4: æ€§èƒ½æµ‹è¯•
    print("æµ‹è¯•4: æ€§èƒ½æµ‹è¯•")
    start_time = time.time()
    large_rdd = sc.parallelize(range(10000000), 100)
    result = large_rdd.map(lambda x: x ** 2).filter(lambda x: x % 10000 == 0).count()
    end_time = time.time()
    print(f"å¤§æ•°æ®é›†å¤„ç†æ—¶é—´: {end_time - start_time:.2f}ç§’, ç»“æœ: {result}")

    spark.stop()
    print("=== æµ‹è¯•å®Œæˆ ===")

if __name__ == "__main__":
    test_spark_cluster()
```

#### Web ç•Œé¢è®¿é—®

- **Spark Master Web UI**: http://hadoop-master1:8080
- **Spark History Server**: http://hadoop-master1:18080
- **Spark Worker Web UI**: http://hadoop-worker1:8081

## é›†ç¾¤éªŒè¯ä¸æµ‹è¯•

### ç³»ç»Ÿé›†æˆæµ‹è¯•

#### å®Œæ•´é›†ç¾¤çŠ¶æ€æ£€æŸ¥

> [!NOTE]
> æ–‡ä»¶å: cluster-health-check.sh

```bash
#!/bin/bash

echo "========== å¤§æ•°æ®å¹³å°å¥åº·æ£€æŸ¥ =========="

echo "1. ZooKeeperé›†ç¾¤çŠ¶æ€:"
docker exec zoo1 /apache-zookeeper-3.7.1-bin/bin/zkServer.sh status
docker exec zoo2 /apache-zookeeper-3.7.1-bin/bin/zkServer.sh status
docker exec zoo3 /apache-zookeeper-3.7.1-bin/bin/zkServer.sh status

echo -e "\n2. Hadoopé›†ç¾¤çŠ¶æ€:"
echo "--- NameNodeçŠ¶æ€ ---"
docker exec hadoop-master1 /opt/hadoop/bin/hdfs haadmin -getServiceState nn1
docker exec hadoop-master2 /opt/hadoop/bin/hdfs haadmin -getServiceState nn2
docker exec hadoop-master3 /opt/hadoop/bin/hdfs haadmin -getServiceState nn3

echo "--- HDFSæŠ¥å‘Š ---"
docker exec hadoop-master1 /opt/hadoop/bin/hdfs dfsadmin -report | head -20

echo "--- YARNèŠ‚ç‚¹ ---"
docker exec hadoop-master1 /opt/hadoop/bin/yarn node -list
```

#### æ•°æ®æµæµ‹è¯•

> [!NOTE]
> æ–‡ä»¶å: data-flow-test.sh

```bash
#!/bin/bash

echo "=== æ•°æ®æµå®Œæ•´æ€§æµ‹è¯• ==="

# 1. HDFSå­˜å‚¨æµ‹è¯•
echo "1. æµ‹è¯•HDFSå­˜å‚¨..."
docker exec hadoop-master1 bash -c "
echo 'Hello Big Data Platform' | /opt/hadoop/bin/hdfs dfs -put - /tmp/test-data.txt
/opt/hadoop/bin/hdfs dfs -cat /tmp/test-data.txt
/opt/hadoop/bin/hdfs dfs -rm /tmp/test-data.txt
"

# 2. HBaseæ•°æ®åº“æµ‹è¯•
echo "2. æµ‹è¯•HBaseæ•°æ®åº“..."
docker exec hadoop-master1 /opt/hbase/bin/hbase shell <<EOF
create 'test_flow', 'data'
put 'test_flow', 'row1', 'data:message', 'Integration Test Success'
get 'test_flow', 'row1'
disable 'test_flow'
drop 'test_flow'
exit
EOF

# 3. Sparkè®¡ç®—æµ‹è¯•
echo "3. æµ‹è¯•Sparkè®¡ç®—..."
docker exec hadoop-master1 /opt/spark/bin/spark-submit \
  --master spark://hadoop-master1:7077 \
  --class org.apache.spark.examples.SparkPi \
  /opt/spark/examples/jars/spark-examples_2.12-3.4.1.jar 5

echo "=== æ•°æ®æµæµ‹è¯•å®Œæˆ ==="
```

### æ•…éšœè½¬ç§»æµ‹è¯•

#### NameNode æ•…éšœè½¬ç§»

> [!NOTE]
> æ–‡ä»¶å: test-namenode-failover.sh

```bash
#!/bin/bash

echo "=== NameNodeæ•…éšœè½¬ç§»æµ‹è¯• ==="

# æŸ¥çœ‹å½“å‰Active NameNode
echo "å½“å‰Active NameNode:"
docker exec hadoop-master1 /opt/hadoop/bin/hdfs haadmin -getServiceState nn1
docker exec hadoop-master2 /opt/hadoop/bin/hdfs haadmin -getServiceState nn2
docker exec hadoop-master3 /opt/hadoop/bin/hdfs haadmin -getServiceState nn3

# æ¨¡æ‹Ÿæ•…éšœè½¬ç§»
echo "æ‰‹åŠ¨åˆ‡æ¢åˆ°nn2..."
docker exec hadoop-master1 /opt/hadoop/bin/hdfs haadmin -transitionToStandby nn1
docker exec hadoop-master2 /opt/hadoop/bin/hdfs haadmin -transitionToActive nn2

# éªŒè¯åˆ‡æ¢ç»“æœ
echo "åˆ‡æ¢åçŠ¶æ€:"
docker exec hadoop-master1 /opt/hadoop/bin/hdfs haadmin -getServiceState nn1
docker exec hadoop-master2 /opt/hadoop/bin/hdfs haadmin -getServiceState nn2
docker exec hadoop-master3 /opt/hadoop/bin/hdfs haadmin -getServiceState nn3

echo "=== æ•…éšœè½¬ç§»æµ‹è¯•å®Œæˆ ==="
```

## ç›‘æ§ä¸è¿ç»´

### ç›‘æ§æŒ‡æ ‡

```mermaid
graph LR
    subgraph "ç³»ç»Ÿç›‘æ§"
        A1[CPUä½¿ç”¨ç‡]
        A2[å†…å­˜ä½¿ç”¨ç‡]
        A3[ç£ç›˜I/O]
        A4[ç½‘ç»œæµé‡]
    end

    subgraph "Hadoopç›‘æ§"
        B1[NameNodeå †å†…å­˜]
        B2[DataNodeå­˜æ´»çŠ¶æ€]
        B3[HDFSä½¿ç”¨ç‡]
        B4[Blockå¤åˆ¶çŠ¶æ€]
    end

    subgraph "HBaseç›‘æ§"
        C1[MasterçŠ¶æ€]
        C2[RegionServerè´Ÿè½½]
        C3[Regionåˆ†å¸ƒ]
        C4[è¯»å†™QPS]
    end

    subgraph "Sparkç›‘æ§"
        D1[åº”ç”¨è¿è¡ŒçŠ¶æ€]
        D2[Executorèµ„æº]
        D3[ä»»åŠ¡æ‰§è¡Œæ—¶é—´]
        D4[æ•°æ®å€¾æ–œæ£€æµ‹]
    end

    subgraph "å‘Šè­¦ç³»ç»Ÿ"
        E1[é‚®ä»¶é€šçŸ¥]
        E2[çŸ­ä¿¡å‘Šè­¦]
        E3[é’‰é’‰/ä¼å¾®]
        E4[ç›‘æ§é¢æ¿]
    end

    A1 --> E1
    B2 --> E2
    C1 --> E3
    D1 --> E4

    style A1 fill:#e8f5e8
    style B1 fill:#fff3e0
    style C1 fill:#e1f5fe
    style D1 fill:#f3e5f5
```

### æ—¥å¿—ç®¡ç†

#### æ—¥å¿—æ”¶é›†è„šæœ¬

> [!NOTE]
> æ–‡ä»¶å: collect-logs.sh

```bash
#!/bin/bash

LOG_DIR="/opt/bigdata-logs/$(date +%Y%m%d)"
mkdir -p $LOG_DIR

echo "=== æ”¶é›†é›†ç¾¤æ—¥å¿— ==="

# æ”¶é›†Hadoopæ—¥å¿—
echo "æ”¶é›†Hadoopæ—¥å¿—..."
docker exec hadoop-master1 tar -czf /tmp/hadoop-logs.tar.gz /opt/hadoop/logs/
docker cp hadoop-master1:/tmp/hadoop-logs.tar.gz $LOG_DIR/

# æ”¶é›†HBaseæ—¥å¿—
echo "æ”¶é›†HBaseæ—¥å¿—..."
docker exec hadoop-master1 tar -czf /tmp/hbase-logs.tar.gz /opt/hbase/logs/
docker cp hadoop-master1:/tmp/hbase-logs.tar.gz $LOG_DIR/

# æ”¶é›†Sparkæ—¥å¿—
echo "æ”¶é›†Sparkæ—¥å¿—..."
docker exec hadoop-master1 tar -czf /tmp/spark-logs.tar.gz /opt/spark/logs/
docker cp hadoop-master1:/tmp/spark-logs.tar.gz $LOG_DIR/

echo "æ—¥å¿—å·²æ”¶é›†åˆ°: $LOG_DIR"
```

### å¤‡ä»½ç­–ç•¥

#### HDFS æ•°æ®å¤‡ä»½

> [!NOTE]
> æ–‡ä»¶å: backup-hdfs.sh

```bash
#!/bin/bash

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/opt/bigdata-backup/hdfs/$BACKUP_DATE"

echo "=== HDFSæ•°æ®å¤‡ä»½ ==="

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å¤‡ä»½HDFSå…ƒæ•°æ®
echo "å¤‡ä»½NameNodeå…ƒæ•°æ®..."
docker exec hadoop-master1 /opt/hadoop/bin/hdfs dfsadmin -saveNamespace
docker exec hadoop-master1 tar -czf /tmp/namenode-backup.tar.gz /data/hdfs/namenode/
docker cp hadoop-master1:/tmp/namenode-backup.tar.gz $BACKUP_DIR/

# å¤‡ä»½é‡è¦æ•°æ®
echo "å¤‡ä»½ç”¨æˆ·æ•°æ®..."
docker exec hadoop-master1 /opt/hadoop/bin/hadoop distcp \
    hdfs://mycluster/user \
    hdfs://mycluster/backup/user_$BACKUP_DATE

echo "HDFSå¤‡ä»½å®Œæˆ: $BACKUP_DIR"
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜è§£å†³

#### 1. é›†ç¾¤å¯åŠ¨é—®é¢˜

#### ZooKeeperè¿æ¥å¤±è´¥

**é—®é¢˜ç°è±¡**: æœåŠ¡æ— æ³•è¿æ¥åˆ° ZooKeeper

**æ’æŸ¥æ­¥éª¤**:

```bash
# æ£€æŸ¥ZooKeeperçŠ¶æ€
docker exec zoo1 /apache-zookeeper-3.7.1-bin/bin/zkServer.sh status

# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
docker exec hadoop-master1 nc -v zoo1 2181

# æ£€æŸ¥é˜²ç«å¢™å’Œç«¯å£
netstat -tulpn | grep 2181
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# é‡å¯ZooKeeperé›†ç¾¤
docker-compose -f zookeeper-compose.yml restart

# æ£€æŸ¥hostsé…ç½®
docker exec hadoop-master1 cat /etc/hosts
```

#### NameNodeæ— æ³•å¯åŠ¨

**é—®é¢˜ç°è±¡**: NameNode å¯åŠ¨å¤±è´¥æˆ–å¤„äº Safe Mode

**æ’æŸ¥æ­¥éª¤**:

```bash
# æ£€æŸ¥JournalNodeçŠ¶æ€
docker exec hadoop-master1 jps | grep JournalNode

# æ£€æŸ¥å…±äº«å­˜å‚¨
docker exec hadoop-master1 ls -la /data/hdfs/journal/node/local/data/

# æŸ¥çœ‹NameNodeæ—¥å¿—
docker exec hadoop-master1 tail -100 /opt/hadoop/logs/hadoop-root-namenode-*.log
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# é‡æ–°åˆå§‹åŒ–ï¼ˆè°¨æ…æ“ä½œï¼‰
docker exec hadoop-master1 /opt/hadoop/bin/hdfs namenode -format -force

# æ‰‹åŠ¨é€€å‡ºå®‰å…¨æ¨¡å¼
docker exec hadoop-master1 /opt/hadoop/bin/hdfs dfsadmin -safemode leave
```

#### 2. æ€§èƒ½é—®é¢˜

#### å†…å­˜ä¸è¶³

**é—®é¢˜ç°è±¡**: OutOfMemoryError å¼‚å¸¸

**è§£å†³æ–¹æ¡ˆ**:

```bash
# è°ƒæ•´JVMå †å†…å­˜ï¼ˆåœ¨å¯¹åº”çš„env.shæ–‡ä»¶ä¸­ï¼‰
export SPARK_DRIVER_MEMORY=2g
export SPARK_EXECUTOR_MEMORY=2g

# æˆ–åœ¨spark-submitä¸­æŒ‡å®š
--driver-memory 2g --executor-memory 2g
```

#### ç½‘ç»œå»¶è¿Ÿé«˜

**é—®é¢˜ç°è±¡**: ä»»åŠ¡æ‰§è¡Œç¼“æ…¢ï¼Œç½‘ç»œè¶…æ—¶

**è§£å†³æ–¹æ¡ˆ**:

```bash
# å¢åŠ ç½‘ç»œè¶…æ—¶é…ç½®
# åœ¨core-site.xmlä¸­æ·»åŠ 
<property>
    <name>ipc.client.connect.timeout</name>
    <value>60000</value>
</property>

# åœ¨spark-defaults.confä¸­æ·»åŠ 
spark.network.timeout 300s
spark.rpc.askTimeout 300s
```

### è¯Šæ–­å·¥å…·

#### é›†ç¾¤è¯Šæ–­è„šæœ¬

> [!NOTE]
> æ–‡ä»¶å: diagnose-cluster.sh

```bash
#!/bin/bash

echo "========== é›†ç¾¤è¯Šæ–­æŠ¥å‘Š =========="
echo "ç”Ÿæˆæ—¶é—´: $(date)"

echo -e "\n1. å®¹å™¨çŠ¶æ€:"
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

echo -e "\n2. èµ„æºä½¿ç”¨æƒ…å†µ:"
echo "--- å†…å­˜ä½¿ç”¨ ---"
docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}"

echo -e "\n3. ç½‘ç»œè¿é€šæ€§:"
hosts=("zoo1" "hadoop-master1" "hadoop-master2" "hadoop-master3")
for host in "${hosts[@]}"; do
    if docker exec hadoop-master1 ping -c 1 $host > /dev/null 2>&1; then
        echo "âœ… $host ç½‘ç»œæ­£å¸¸"
    else
        echo "âŒ $host ç½‘ç»œå¼‚å¸¸"
    fi
done

echo -e "\n4. å…³é”®è¿›ç¨‹æ£€æŸ¥:"
containers=("hadoop-master1" "hadoop-master2" "hadoop-master3" "hadoop-worker1")
for container in "${containers[@]}"; do
    echo "--- $container ---"
    docker exec $container jps 2>/dev/null || echo "å®¹å™¨ä¸å¯è®¿é—®"
done

echo -e "\n5. ç£ç›˜ç©ºé—´:"
docker exec hadoop-master1 df -h | grep -E "(Filesystem|/data|/opt|/tmp)"

echo -e "\n========== è¯Šæ–­å®Œæˆ =========="
```

## æ€§èƒ½ä¼˜åŒ–

### ç¡¬ä»¶ä¼˜åŒ–å»ºè®®

#### å­˜å‚¨ä¼˜åŒ–

```mermaid
graph LR
    subgraph "å­˜å‚¨å±‚ä¼˜åŒ–"
        A1[SSDç”¨äºå…ƒæ•°æ®]
        A2[æœºæ¢°ç›˜ç”¨äºæ•°æ®]
        A3[å†…å­˜ç¼“å­˜]
        A4[å‹ç¼©ç®—æ³•]
    end

    subgraph "ç½‘ç»œä¼˜åŒ–"
        B1[ä¸‡å…†ç½‘å¡]
        B2[äº¤æ¢æœºä¼˜åŒ–]
        B3[ç½‘ç»œéš”ç¦»]
        B4[å¤šç½‘å¡ç»‘å®š]
    end

    subgraph "è®¡ç®—ä¼˜åŒ–"
        C1[CPUæ ¸å¿ƒæ•°]
        C2[å†…å­˜å®¹é‡]
        C3[NUMAé…ç½®]
        C4[è¶…çº¿ç¨‹è®¾ç½®]
    end

    A1 --> D[æ€§èƒ½æå‡]
    B1 --> D
    C1 --> D

    style D fill:#ffcdd2
```

### è½¯ä»¶è°ƒä¼˜å‚æ•°

#### Hadoop è°ƒä¼˜

```xml
<!-- HDFSæ€§èƒ½ä¼˜åŒ–é…ç½® -->
<configuration>
    <!-- å¢åŠ æ•°æ®ä¼ è¾“çº¿ç¨‹ -->
    <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>8192</value>
    </property>

    <!-- å¢åŠ NameNodeå¤„ç†çº¿ç¨‹ -->
    <property>
        <name>dfs.namenode.handler.count</name>
        <value>100</value>
    </property>

    <!-- ä¼˜åŒ–å—å¤§å° -->
    <property>
        <name>dfs.blocksize</name>
        <value>268435456</value> <!-- 256MB -->
    </property>

    <!-- å¯ç”¨çŸ­è·¯è¯»å– -->
    <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>true</value>
    </property>
</configuration>
```

#### HBase è°ƒä¼˜

```xml
<!-- HBaseæ€§èƒ½ä¼˜åŒ–é…ç½® -->
<configuration>
    <!-- å¢åŠ RegionServerå¤„ç†çº¿ç¨‹ -->
    <property>
        <name>hbase.regionserver.handler.count</name>
        <value>100</value>
    </property>

    <!-- ä¼˜åŒ–å—ç¼“å­˜ -->
    <property>
        <name>hbase.bucketcache.size</name>
        <value>2048</value>
    </property>

    <!-- é¢„åˆ†åŒºè®¾ç½® -->
    <property>
        <name>hbase.hregion.presplit.size</name>
        <value>1073741824</value> <!-- 1GB -->
    </property>

    <!-- å‹ç¼©é…ç½® -->
    <property>
        <name>hbase.hregion.majorcompaction</name>
        <value>604800000</value> <!-- 7å¤© -->
    </property>
</configuration>
```

#### Spark è°ƒä¼˜

```properties
# Sparkæ€§èƒ½ä¼˜åŒ–é…ç½®

# å†…å­˜é…ç½®
spark.driver.memory              4g
spark.driver.maxResultSize       2g
spark.executor.memory            4g
spark.executor.memoryFraction    0.8

# å¹¶è¡Œåº¦é…ç½®
spark.default.parallelism        200
spark.sql.shuffle.partitions     200

# ç½‘ç»œé…ç½®
spark.network.timeout           300s
spark.rpc.askTimeout            300s
spark.rpc.lookupTimeout         300s

# å­˜å‚¨é…ç½®
spark.storage.memoryFraction     0.6
spark.storage.unrollFraction     0.2

# GCé…ç½®
spark.executor.extraJavaOptions  -XX:+UseG1GC -XX:MaxGCPauseMillis=200
```

## æ€»ç»“

é€šè¿‡æœ¬æŒ‡å—ï¼Œæˆ‘ä»¬æˆåŠŸæ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§å¤§æ•°æ®å¤„ç†å¹³å°ï¼Œæ¶µç›–äº†ä»å­˜å‚¨åˆ°è®¡ç®—çš„å®Œæ•´æŠ€æœ¯æ ˆã€‚

### æŠ€æœ¯æ¶æ„å›é¡¾

```mermaid
journey
    title å¤§æ•°æ®å¹³å°å»ºè®¾å†ç¨‹
    section åŸºç¡€è®¾æ–½
      Dockerç¯å¢ƒå‡†å¤‡: 5: è¿ç»´
      ç½‘ç»œé…ç½®: 5: è¿ç»´
      ZooKeeperéƒ¨ç½²: 4: è¿ç»´
    section å­˜å‚¨å±‚
      Hadoop HDFSéƒ¨ç½²: 4: è¿ç»´
      é«˜å¯ç”¨é…ç½®: 3: è¿ç»´
      æ•…éšœè½¬ç§»æµ‹è¯•: 4: è¿ç»´
    section æ•°æ®åº“å±‚
      HBaseå®‰è£…é…ç½®: 3: è¿ç»´
      Master HAé…ç½®: 3: è¿ç»´
      æ•°æ®æ“ä½œéªŒè¯: 5: å¼€å‘
    section è®¡ç®—å±‚
      Sparké›†ç¾¤éƒ¨ç½²: 4: è¿ç»´
      Pythonç¯å¢ƒé…ç½®: 4: å¼€å‘
      ä»»åŠ¡è°ƒåº¦æµ‹è¯•: 5: å¼€å‘
    section è¿ç»´ç›‘æ§
      ç›‘æ§ä½“ç³»æ­å»º: 3: è¿ç»´
      æ•…éšœæ’é™¤: 4: è¿ç»´
      æ€§èƒ½ä¼˜åŒ–: 4: è¿ç»´
```

### å¹³å°èƒ½åŠ›çŸ©é˜µ

| èƒ½åŠ›ç»´åº¦           | Hadoop    | HBase      | Spark    | æ•´ä½“å¹³å°   |
| ------------------ | --------- | ---------- | -------- | ---------- |
| **å­˜å‚¨èƒ½åŠ›** | PB çº§å­˜å‚¨ | ç»“æ„åŒ–å­˜å‚¨ | å†…å­˜è®¡ç®— | â­â­â­â­â­ |
| **è®¡ç®—èƒ½åŠ›** | æ‰¹å¤„ç†    | éšæœºè¯»å†™   | æ‰¹æµç»Ÿä¸€ | â­â­â­â­â­ |
| **å¯é æ€§**   | é«˜å¯ç”¨    | æ•…éšœè½¬ç§»   | å®¹é”™æœºåˆ¶ | â­â­â­â­â­ |
| **æ‰©å±•æ€§**   | æ°´å¹³æ‰©å±•  | è‡ªåŠ¨åˆ†åŒº   | å¼¹æ€§ä¼¸ç¼© | â­â­â­â­â­ |
| **æ€§èƒ½**     | é«˜åå    | ä½å»¶è¿Ÿ     | å†…å­˜è®¡ç®— | â­â­â­â­   |
| **æ˜“ç”¨æ€§**   | é…ç½®å¤æ‚  | å­¦ä¹ æ›²çº¿   | API ä¸°å¯Œ | â­â­â­     |

### åº”ç”¨åœºæ™¯å»ºè®®

1. **æ•°æ®ä»“åº“**: ä½¿ç”¨ HDFS+Spark æ„å»ºç¦»çº¿æ•°æ®å¤„ç†ç®¡é“
2. **å®æ—¶åˆ†æ**: ç»“åˆ HBase+Spark Streaming å®ç°å®æ—¶æ•°æ®åˆ†æ
3. **æœºå™¨å­¦ä¹ **: åˆ©ç”¨ Spark MLlib è¿›è¡Œå¤§è§„æ¨¡æœºå™¨å­¦ä¹ è®­ç»ƒ
4. **æ—¥å¿—åˆ†æ**: é€šè¿‡å®Œæ•´é“¾è·¯å¤„ç†æµ·é‡æ—¥å¿—æ•°æ®

### è¿›é˜¶å­¦ä¹ è·¯å¾„

- ğŸ”® **å®¹å™¨ç¼–æ’**: Kubernetes éƒ¨ç½²å¤§æ•°æ®å¹³å°
- ğŸ”® **æµå¼è®¡ç®—**: Flink é›†æˆä¸å¯¹æ¯”åˆ†æ
- ğŸ”® **æ•°æ®æ²»ç†**: å…ƒæ•°æ®ç®¡ç†ä¸è¡€ç¼˜å…³ç³»
- ğŸ”® **æœºå™¨å­¦ä¹ **: MLOps å¹³å°å»ºè®¾
- ğŸ”® **äº‘åŸç”Ÿ**: äº‘ä¸Šå¤§æ•°æ®è§£å†³æ–¹æ¡ˆ

### ç”Ÿäº§å®è·µå»ºè®®

1. **åˆ†é˜¶æ®µéƒ¨ç½²**: å…ˆæ­å»ºæµ‹è¯•ç¯å¢ƒï¼ŒéªŒè¯åå†ä¸Šç”Ÿäº§
2. **å®¹é‡è§„åˆ’**: æ ¹æ®ä¸šåŠ¡éœ€æ±‚åˆç†è§„åˆ’ç¡¬ä»¶èµ„æº
3. **ç›‘æ§å‘Šè­¦**: å»ºç«‹å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦ä½“ç³»
4. **å¤‡ä»½ç­–ç•¥**: åˆ¶å®šæ•°æ®å¤‡ä»½å’Œæ¢å¤è®¡åˆ’
5. **æ–‡æ¡£ç»´æŠ¤**: ä¿æŒæŠ€æœ¯æ–‡æ¡£å’Œæ“ä½œæ‰‹å†Œæ›´æ–°

---

> ğŸ’¡ **æœ€ä½³å®è·µ**: åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šçš„å¤§æ•°æ®å¹³å°ç®¡ç†å·¥å…·ï¼ˆå¦‚ Ambariã€Cloudera Managerï¼‰æ¥ç®€åŒ–è¿ç»´å·¥ä½œã€‚

> ğŸ¯ **æŠ€æœ¯å‘å±•**: å…³æ³¨äº‘åŸç”Ÿã€serverless ç­‰æ–°æŠ€æœ¯è¶‹åŠ¿ï¼Œé€‚æ—¶è¿›è¡Œå¹³å°å‡çº§å’ŒæŠ€æœ¯æ ˆä¼˜åŒ–ã€‚

> âš ï¸ **å®‰å…¨æé†’**: ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ—¶ï¼Œè¯·åŠ¡å¿…é…ç½® Kerberos è®¤è¯ã€SSL åŠ å¯†ç­‰å®‰å…¨æªæ–½ã€‚

**å¦‚æœæœ¬æŒ‡å—å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª Star æ”¯æŒï¼å¦‚æœ‰é—®é¢˜æ¬¢è¿æ Issue äº¤æµè®¨è®ºã€‚** â­ï¸
