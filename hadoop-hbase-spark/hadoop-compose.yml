# 定义公共配置锚点
x-hadoop: &hadoop-common
  image: big-data-components:1.0
  stdin_open: true
  tty: true
  command: sh -c "/usr/sbin/sshd -D"
  networks:
    zookeeper-cluster:
  volumes: &hadoop-volumes
    - type: bind
      source: ~/opt/docker-data/hadoop-hbase-spark/hadoop
      target: /opt/hadoop
    - type: bind
      source: ~/opt/docker-data/hadoop-hbase-spark/hbase
      target: /opt/hbase
    - type: bind
      source: ~/opt/docker-data/hadoop-hbase-spark/spark
      target: /opt/spark
    - /etc/localtime:/etc/localtime:ro
    - /etc/timezone:/etc/timezone:ro
  environment: &hadoop-env
    JAVA_HOME: "/usr/lib/jvm/java-8-openjdk-amd64"
    HADOOP_HOME: "/opt/hadoop"
    HADOOP_COMMON_HOME: "/opt/hadoop"
    HADOOP_HDFS_HOME: "/opt/hadoop"
    HADOOP_MAPRED_HOME: "/opt/hadoop"
    YARN_HOME: "/opt/hadoop"
    HBASE_HOME: "/opt/hbase"
    SPARK_HOME: "/opt/spark"
    PYTHON_HOME: "/opt/miniconda3/envs/pyspark"
    CONDA_DEFAULT_ENV: "pyspark"
    HADOOP_CONF_DIR: "/opt/hadoop/etc/hadoop"
    HBASE_CONF_DIR: "/opt/hbase/conf"
    SPARK_CONF_DIR: "/opt/spark/conf"
    PYSPARK_PYTHON: "/opt/miniconda3/envs/pyspark/bin/python"
    PYSPARK_DRIVER_PYTHON: "/opt/miniconda3/envs/pyspark/bin/python"
    PYTHONPATH: "/opt/miniconda3/envs/pyspark/lib/python3.8/site-packages"
    PYTHONIOENCODING: "utf-8"
    CLASSPATH: ".:/usr/lib/jvm/java-8-openjdk-amd64/lib/dt.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar"
    PATH: "/opt/miniconda3/envs/pyspark/bin:/opt/miniconda3/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin:/opt/hadoop/bin:/opt/hadoop/sbin:/opt/hbase/bin:/opt/spark/bin:/opt/spark/sbin:${PATH}"
    # Hadoop用户权限配置 - 解决root用户运行错误
    HDFS_NAMENODE_USER: "root"
    HDFS_DATANODE_USER: "root"
    HDFS_JOURNALNODE_USER: "root"
    HDFS_ZKFC_USER: "root"
    YARN_RESOURCEMANAGER_USER: "root"
    YARN_NODEMANAGER_USER: "root"
    YARN_PROXYSERVER_USER: "root"
    MAPRED_HISTORYSERVER_USER: "root"

services:
  # Hadoop 主节点配置
  hadoop-master1: &hadoop-master
    <<: *hadoop-common
    container_name: hadoop-master1
    hostname: hadoop-master1
    volumes: *hadoop-volumes
    environment: *hadoop-env
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.20
    restart: no
    
  hadoop-master2:
    <<: *hadoop-master
    container_name: hadoop-master2
    hostname: hadoop-master2
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.21
    restart: no
    
  hadoop-master3:
    <<: *hadoop-master
    container_name: hadoop-master3
    hostname: hadoop-master3
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.22
    restart: no
    
  hadoop-worker1:
    <<: *hadoop-common
    container_name: hadoop-worker1
    hostname: hadoop-worker1
    environment: *hadoop-env
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.23
    restart: no
    
  hadoop-worker2:
    <<: *hadoop-common
    container_name: hadoop-worker2
    hostname: hadoop-worker2
    environment: *hadoop-env
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.24
    restart: no
    
  hadoop-worker3:
    <<: *hadoop-common
    container_name: hadoop-worker3
    hostname: hadoop-worker3
    environment: *hadoop-env
    networks:
      zookeeper-cluster:
        ipv4_address: 10.10.1.25
    restart: no

networks:
  zookeeper-cluster:
    name: zookeeper-cluster
    external: true
    ipam:
      config:
        - subnet: "10.10.1.0/24"
