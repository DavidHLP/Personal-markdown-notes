#!/bin/bash
# spark-init.sh - Spark åˆå§‹åŒ–è„šæœ¬
# ä¼˜åŒ–ç‰ˆæœ¬ï¼šæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
ORANGE='\033[0;33m'
NC='\033[0m' # No Color

# é…ç½®å˜é‡
CONTAINER_NAME="hadoop-master1"
HDFS_SPARK_DIR="/sparklog"
HADOOP_BIN="/opt/hadoop/bin/hdfs"
SPARK_PERMISSIONS="777"

# æ‰“å°å¸¦é¢œè‰²çš„æ¶ˆæ¯
print_message() {
    local color=$1
    local message=$2
    echo -e "${color}${message}${NC}"
}

# æ‰“å°æˆåŠŸæ¶ˆæ¯
print_success() {
    print_message $GREEN "âœ“ $1"
}

# æ‰“å°è­¦å‘Šæ¶ˆæ¯
print_warning() {
    print_message $YELLOW "âš  $1"
}

# æ‰“å°é”™è¯¯æ¶ˆæ¯
print_error() {
    print_message $RED "âœ— $1"
}

# æ‰“å°ä¿¡æ¯æ¶ˆæ¯
print_info() {
    print_message $BLUE "â„¹ $1"
}

# æ‰“å°å®‰å…¨è­¦å‘Š
print_security_warning() {
    print_message $ORANGE "ğŸ”’ å®‰å…¨è­¦å‘Š: $1"
}

# æ£€æŸ¥Dockerå®¹å™¨æ˜¯å¦è¿è¡Œ
check_container() {
    print_info "æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€..."
    
    if ! command -v docker &> /dev/null; then
        print_error "Docker æœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­"
        return 1
    fi
    
    if ! docker ps | grep -q "$CONTAINER_NAME"; then
        print_error "å®¹å™¨ '$CONTAINER_NAME' æœªè¿è¡Œ"
        print_info "è¯·å…ˆå¯åŠ¨Hadoopé›†ç¾¤å®¹å™¨"
        return 1
    fi
    
    print_success "å®¹å™¨ '$CONTAINER_NAME' æ­£åœ¨è¿è¡Œ"
    return 0
}

# æ£€æŸ¥HDFSæœåŠ¡çŠ¶æ€
check_hdfs_status() {
    print_info "æ£€æŸ¥HDFSæœåŠ¡çŠ¶æ€..."
    
    if ! docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -ls / &>/dev/null; then
        print_error "HDFSæœåŠ¡æœªæ­£å¸¸è¿è¡Œ"
        print_info "è¯·ç¡®ä¿Hadoopé›†ç¾¤å·²æ­£ç¡®å¯åŠ¨"
        return 1
    fi
    
    print_success "HDFSæœåŠ¡æ­£å¸¸"
    return 0
}

# æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
check_directory_exists() {
    local dir_path=$1
    docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -test -d "$dir_path" 2>/dev/null
}

# æ˜¾ç¤ºæƒé™å®‰å…¨è­¦å‘Š
show_security_warning() {
    echo
    print_security_warning "æƒé™è®¾ç½®è­¦å‘Š"
    echo
    print_message $ORANGE "å³å°†è®¾ç½®ç›®å½•æƒé™ä¸º 777 (rwxrwxrwx)"
    print_message $ORANGE "è¿™æ„å‘³ç€ï¼š"
    echo "  - æ‰€æœ‰ç”¨æˆ·éƒ½å¯ä»¥è¯»å–ã€å†™å…¥å’Œæ‰§è¡Œ"
    echo "  - è¿™å¯èƒ½å­˜åœ¨å®‰å…¨é£é™©"
    echo "  - å»ºè®®ä»…åœ¨å¼€å‘ç¯å¢ƒä¸­ä½¿ç”¨"
    echo
    print_message $YELLOW "æ˜¯å¦ç»§ç»­è®¾ç½® 777 æƒé™ï¼Ÿ[y/N]"
    read -t 15 -n 1 -r
    echo
    
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        print_info "ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œå°†è®¾ç½®æ›´å®‰å…¨çš„ 755 æƒé™"
        SPARK_PERMISSIONS="755"
        return 1
    fi
    
    print_warning "ç”¨æˆ·ç¡®è®¤ä½¿ç”¨ 777 æƒé™"
    return 0
}

# åˆ›å»ºHDFSç›®å½•
create_hdfs_directory() {
    print_info "åˆ›å»ºHDFSç›®å½•: $HDFS_SPARK_DIR"
    
    if check_directory_exists "$HDFS_SPARK_DIR"; then
        print_warning "ç›®å½• '$HDFS_SPARK_DIR' å·²å­˜åœ¨"
        return 0
    fi
    
    if docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -mkdir -p "$HDFS_SPARK_DIR"; then
        print_success "ç›®å½•åˆ›å»ºæˆåŠŸ: $HDFS_SPARK_DIR"
        return 0
    else
        print_error "ç›®å½•åˆ›å»ºå¤±è´¥: $HDFS_SPARK_DIR"
        return 1
    fi
}

# è®¾ç½®ç›®å½•æƒé™
set_directory_permissions() {
    print_info "è®¾ç½®ç›®å½•æƒé™: $HDFS_SPARK_DIR ($SPARK_PERMISSIONS)"
    
    if docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -chmod "$SPARK_PERMISSIONS" "$HDFS_SPARK_DIR"; then
        print_success "æƒé™è®¾ç½®æˆåŠŸ: $SPARK_PERMISSIONS"
        return 0
    else
        print_error "æƒé™è®¾ç½®å¤±è´¥"
        return 1
    fi
}

# éªŒè¯ç›®å½•æƒé™
verify_permissions() {
    print_info "éªŒè¯ç›®å½•æƒé™..."
    
    local permissions=$(docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -ls / 2>/dev/null | grep "sparklog" | awk '{print $1}')
    
    if [[ -n "$permissions" ]]; then
        print_success "æƒé™éªŒè¯é€šè¿‡: $permissions"
        
        # æ£€æŸ¥æ˜¯å¦ä¸º777æƒé™å¹¶å†æ¬¡æé†’å®‰å…¨é£é™©
        if [[ $permissions =~ rwxrwxrwx ]]; then
            print_security_warning "å½“å‰ä½¿ç”¨777æƒé™ï¼Œè¯·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è€ƒè™‘æ›´å®‰å…¨çš„æƒé™è®¾ç½®"
        fi
        return 0
    else
        print_warning "æ— æ³•è·å–æƒé™ä¿¡æ¯"
        return 1
    fi
}

# æ˜¾ç¤ºç›®å½•ä¿¡æ¯
show_directory_info() {
    print_info "æ˜¾ç¤ºSparkæ—¥å¿—ç›®å½•ä¿¡æ¯:"
    echo
    print_message $CYAN "ç›®å½•è¯¦æƒ…:"
    docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -ls -d "$HDFS_SPARK_DIR" 2>/dev/null || {
        print_error "æ— æ³•è·å–ç›®å½•ä¿¡æ¯"
        return 1
    }
    echo
}

# æµ‹è¯•ç›®å½•è®¿é—®æƒé™
test_directory_access() {
    print_info "æµ‹è¯•ç›®å½•è®¿é—®æƒé™..."
    
    # å°è¯•åœ¨ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªæµ‹è¯•æ–‡ä»¶
    local test_file="$HDFS_SPARK_DIR/spark-init-test-$(date +%s)"
    
    if docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -touchz "$test_file" 2>/dev/null; then
        print_success "ç›®å½•å†™å…¥æµ‹è¯•æˆåŠŸ"
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -rm "$test_file" 2>/dev/null
        return 0
    else
        print_error "ç›®å½•å†™å…¥æµ‹è¯•å¤±è´¥"
        return 1
    fi
}

# æ˜¾ç¤ºSparké…ç½®å»ºè®®
show_spark_config_tips() {
    print_info "Sparké…ç½®å»ºè®®:"
    echo
    print_message $CYAN "åœ¨Sparké…ç½®ä¸­è®¾ç½®ä»¥ä¸‹å‚æ•°ï¼š"
    echo "  spark.eventLog.enabled=true"
    echo "  spark.eventLog.dir=hdfs://namenode:9000$HDFS_SPARK_DIR"
    echo "  spark.history.fs.logDirectory=hdfs://namenode:9000$HDFS_SPARK_DIR"
    echo
    print_message $CYAN "è¿™å°†å¯ç”¨Sparkäº‹ä»¶æ—¥å¿—è®°å½•åŠŸèƒ½"
}

# æ˜¾ç¤ºHDFSæ¦‚è§ˆ
show_hdfs_overview() {
    print_info "HDFSæ–‡ä»¶ç³»ç»Ÿæ¦‚è§ˆ:"
    echo
    print_message $CYAN "æ ¹ç›®å½•å†…å®¹:"
    docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -ls / 2>/dev/null || {
        print_error "æ— æ³•è·å–HDFSä¿¡æ¯"
        return 1
    }
    echo
}

# ä¸»å‡½æ•°
main() {
    # æ˜¾ç¤ºæ ‡é¢˜
    echo
    print_message $PURPLE "========================================"
    print_message $PURPLE "    Spark HDFS åˆå§‹åŒ–è„šæœ¬ (ä¼˜åŒ–ç‰ˆ)"
    print_message $PURPLE "========================================"
    echo
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print_message $CYAN "é…ç½®ä¿¡æ¯:"
    echo "  - å®¹å™¨åç§°: $CONTAINER_NAME"
    echo "  - Sparkæ—¥å¿—ç›®å½•: $HDFS_SPARK_DIR"
    echo "  - HadoopäºŒè¿›åˆ¶: $HADOOP_BIN"
    echo "  - é»˜è®¤æƒé™: $SPARK_PERMISSIONS"
    echo
    
    # æ˜¾ç¤ºæƒé™å®‰å…¨è­¦å‘Š
    if [[ "$SPARK_PERMISSIONS" == "777" ]]; then
        show_security_warning
    fi
    
    # æ‰§è¡Œæ£€æŸ¥å’Œåˆå§‹åŒ–æ­¥éª¤
    local steps=(
        "check_container:æ£€æŸ¥Dockerå®¹å™¨"
        "check_hdfs_status:æ£€æŸ¥HDFSçŠ¶æ€"
        "create_hdfs_directory:åˆ›å»ºSparkæ—¥å¿—ç›®å½•"
        "set_directory_permissions:è®¾ç½®ç›®å½•æƒé™"
        "verify_permissions:éªŒè¯æƒé™è®¾ç½®"
        "test_directory_access:æµ‹è¯•ç›®å½•è®¿é—®"
        "show_directory_info:æ˜¾ç¤ºç›®å½•ä¿¡æ¯"
    )
    
    local total_steps=${#steps[@]}
    local current_step=0
    local failed_steps=()
    
    for step_info in "${steps[@]}"; do
        current_step=$((current_step + 1))
        local func_name="${step_info%%:*}"
        local description="${step_info##*:}"
        
        print_message $BLUE "[$current_step/$total_steps] $description"
        
        if ! $func_name; then
            failed_steps+=("$description")
            print_error "æ­¥éª¤å¤±è´¥: $description"
            
            # å¯¹äºå…³é”®æ­¥éª¤å¤±è´¥ï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
            if [[ "$func_name" == "check_container" || "$func_name" == "check_hdfs_status" ]]; then
                print_message $YELLOW "å…³é”®æ­¥éª¤å¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ[y/N]"
                read -t 10 -n 1 -r
                echo
                if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                    print_error "ç”¨æˆ·é€‰æ‹©é€€å‡º"
                    exit 1
                fi
            fi
        fi
        echo
    done
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    if [ ${#failed_steps[@]} -eq 0 ]; then
        print_message $GREEN "========================================"
        print_message $GREEN "       Spark åˆå§‹åŒ–æˆåŠŸå®Œæˆï¼"
        print_message $GREEN "========================================"
        echo
        print_success "æ‰€æœ‰æ­¥éª¤å‡å·²æˆåŠŸæ‰§è¡Œ"
    else
        print_message $YELLOW "========================================"
        print_message $YELLOW "    Spark åˆå§‹åŒ–å®Œæˆï¼ˆæœ‰è­¦å‘Šï¼‰"
        print_message $YELLOW "========================================"
        echo
        print_warning "ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œå¤±è´¥æˆ–æœ‰è­¦å‘Š:"
        for failed_step in "${failed_steps[@]}"; do
            print_error "- $failed_step"
        done
    fi
    
    # æ˜¾ç¤ºé…ç½®å»ºè®®
    echo
    show_spark_config_tips
    
    echo
    print_info "å¯é€‰æ“ä½œ:"
    print_message $YELLOW "1. æ˜¾ç¤ºHDFSæ¦‚è§ˆ [1]"
    print_message $YELLOW "2. æ˜¾ç¤ºç›®å½•å¤§å° [2]"
    print_message $YELLOW "3. è·³è¿‡ [ä»»æ„é”®]"
    read -t 8 -n 1 -r
    echo
    
    case $REPLY in
        1)
            show_hdfs_overview
            ;;
        2)
            print_info "ç›®å½•å¤§å°ä¿¡æ¯:"
            docker exec "$CONTAINER_NAME" "$HADOOP_BIN" dfs -du -h "$HDFS_SPARK_DIR" 2>/dev/null || print_warning "ç›®å½•ä¸ºç©ºæˆ–æ— æ³•è®¿é—®"
            ;;
        *)
            print_info "è·³è¿‡å¯é€‰æ“ä½œ"
            ;;
    esac
    
    echo
    print_message $CYAN "æç¤º: Sparkç°åœ¨å¯ä»¥ä½¿ç”¨HDFSç›®å½• '$HDFS_SPARK_DIR' å­˜å‚¨äº‹ä»¶æ—¥å¿—"
    print_message $CYAN "ä¸‹ä¸€æ­¥: é…ç½®Sparkä»¥å¯ç”¨äº‹ä»¶æ—¥å¿—è®°å½•"
}

# é”™è¯¯å¤„ç†
set -e
trap 'print_error "è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä½ç½®: $BASH_COMMAND"' ERR

# å…è®¸è„šæœ¬åœ¨æŸäº›å‘½ä»¤å¤±è´¥æ—¶ç»§ç»­è¿è¡Œ
set +e

# è¿è¡Œä¸»å‡½æ•°
main "$@"